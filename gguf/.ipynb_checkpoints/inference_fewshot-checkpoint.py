"""
Qwen3-Next-80B-A3B-Thinking ëª¨ë¸ì„ ì‚¬ìš©í•œ Few-shot ê°ê´€ì‹ ë¬¸ì œ ì¶”ë¡  ìŠ¤í¬ë¦½íŠ¸ (í† í° ê¸¸ì´ ì œí•œ í¬í•¨)

ì„¤ì • ë°©ë²•:
1. prepare_fewshot.pyë¥¼ ì‹¤í–‰í•˜ì—¬ test_with_fewshot.csv ìƒì„±
2. llama-server ì‹¤í–‰ (ë³„ë„ í„°ë¯¸ë„)
3. ì´ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰

í•„ìš” íŒŒì¼:
- test_with_fewshot.csv (prepare_fewshot.pyë¡œ ìƒì„±)
- data/output.csv

ì¶”ë¡ ì—”ì§„ ì‹¤í–‰ (ë³„ë„ í„°ë¯¸ë„):
export LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH
./llama.cpp/build/bin/llama-server -m ./models/Qwen3-Next-80B-A3B-Thinking-UD-Q3_K_XL.gguf -c 32768 -np 2 -cb -fa on --port 8000 --host 0.0.0.0

"""

import pandas as pd
import asyncio
import ast
import time
import os
import re
from collections import Counter
from openai import AsyncOpenAI
from tqdm import tqdm

# ===== í† í° ì„¤ì • =====
# ì¶”ë¡  ì†ë„ vs í’ˆì§ˆ íŠ¸ë ˆì´ë“œì˜¤í”„
# - ë¹ ë¥¸ ì¶”ë¡ : CTX_SIZE=16384, MAX_TOKENS=4000 (5-8ì‹œê°„)
# - ê· í˜•ì¡íŒ: CTX_SIZE=32768, MAX_TOKENS=6000 (12-18ì‹œê°„) â† í˜„ì¬ ì„¤ì •
# - ê³ í’ˆì§ˆ: CTX_SIZE=51000, MAX_TOKENS=12000 (33ì‹œê°„)
CTX_SIZE = 32768          # llama-server -c ê°’ (ê· í˜•ì¡íŒ ì„¤ì •)
MAX_TOKENS = 6000         # ì¶œë ¥ ìµœëŒ€ í† í°
SAFETY_MARGIN = 1024      # ì—¬ìœ ë¶„
MAX_INPUT_TOKENS = CTX_SIZE - MAX_TOKENS - SAFETY_MARGIN  # 25744 í† í°

# Few-shot ë¹„ìœ¨: ì…ë ¥ì˜ 60%ë¥¼ few-shotì— í• ë‹¹ (ì ˆë°˜ë³´ë‹¤ ì¡°ê¸ˆ ë” ë§ì´)
FEWSHOT_RATIO = 0.6

# Loop ì„¤ì •: seed ë‹¤ì–‘ì„±ì„ ìœ„í•œ ë°˜ë³µ íšŸìˆ˜
NUM_LOOPS = 2  # seed ê¸°ë°˜ ì•™ìƒë¸”ì„ ìœ„í•œ ë£¨í”„ (temperature=1.0ì´ë¯€ë¡œ ë‹¤ì–‘ì„± í™•ë³´)

# ê°„ë‹¨í•œ í† í° ì¶”ì • (1 í† í° â‰ˆ 4 ê¸€ì)
def estimate_tokens(text):
    """í…ìŠ¤íŠ¸ì˜ ëŒ€ëµì ì¸ í† í° ìˆ˜ ì¶”ì • (í•œê¸€/ì˜ì–´ í˜¼í•©)"""
    return len(text) // 4

def truncate_text(text, max_chars):
    """í…ìŠ¤íŠ¸ë¥¼ ìµœëŒ€ ê¸€ì ìˆ˜ë¡œ ìë¥´ê¸°"""
    if len(text) <= max_chars:
        return text
    return text[:max_chars] + "...[ì´í•˜ ìƒëµ]"


# ë°ì´í„° ë¡œë“œ
df_test = pd.read_csv('test_with_fewshot_fixed.csv')

# Truncation ê²½ê³  ë¡œê·¸ ì €ì¥ìš©
truncation_warnings = []

# OpenAI í´ë¼ì´ì–¸íŠ¸ ì„¤ì • (ë¡œì»¬ llama.cpp ì„œë²„ ì‚¬ìš©)
client = AsyncOpenAI(
    base_url="http://localhost:8000/v1",
    api_key="sk-no-key-required"
)

# System Prompts
system_msg_0 = """ë‹¹ì‹ ì€ ê°ê´€ì‹ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” í•™ìƒì…ë‹ˆë‹¤. ì œì‹œë¬¸ê³¼ ì§ˆë¬¸ìœ¼ë¡œ ì´ë£¨ì–´ì§„ ë¬¸ì œê°€ ì£¼ì–´ì§‘ë‹ˆë‹¤.
ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ ì‚¬ê³ ê³¼ì •ì„ step-by-stepìœ¼ë¡œ ìì„¸íˆ ëª…ì‹œí•˜ë©´ì„œ ì¶”ë¡ ì„ ìˆ˜í–‰í•´ì£¼ì„¸ìš”. ì •ë‹µì— ì´ë¥´ëŠ” ê³¼ì •ì€ ë§¤ìš° ë…¼ë¦¬ì ìœ¼ë¡œ ì‘ì„±ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.
í’€ì´ê³¼ì •ì´ ëë‚˜ë©´ ë§¨ ë§ˆì§€ë§‰ ì¤„ì—ëŠ” ì•„ë˜ì˜ í˜•ì‹ìœ¼ë¡œ ì •ë‹µì„ ê¸°ì¬í•´ì£¼ì„¸ìš”.
{"ì •ë‹µ": "ë²ˆí˜¸"}"""


system_msg_1 = """ë‹¹ì‹ ì€ ê°ê´€ì‹ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” í•™ìƒì…ë‹ˆë‹¤. ì•„ë˜ì— ìœ ì‚¬í•œ ë¬¸ì œì˜ ì˜ˆì‹œê°€ ì œê³µë©ë‹ˆë‹¤.

## ì˜ˆì‹œ ë¬¸ì œë“¤

{few_shot_examples}

---

## ì´ì œ ë‹¤ìŒ ë¬¸ì œë¥¼ í’€ì–´ì£¼ì„¸ìš”

ì œì‹œë¬¸ê³¼ ì§ˆë¬¸ìœ¼ë¡œ ì´ë£¨ì–´ì§„ ë¬¸ì œê°€ ì£¼ì–´ì§‘ë‹ˆë‹¤.
ìœ„ ì˜ˆì‹œë“¤ì„ ì°¸ê³ í•˜ì—¬ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ ì‚¬ê³ ê³¼ì •ì„ step-by-stepìœ¼ë¡œ ìì„¸íˆ ëª…ì‹œí•˜ë©´ì„œ ì¶”ë¡ ì„ ìˆ˜í–‰í•´ì£¼ì„¸ìš”.
ì •ë‹µì— ì´ë¥´ëŠ” ê³¼ì •ì€ ë§¤ìš° ë…¼ë¦¬ì ìœ¼ë¡œ ì‘ì„±ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.
í’€ì´ê³¼ì •ì´ ëë‚˜ë©´ ë§¨ ë§ˆì§€ë§‰ ì¤„ì—ëŠ” ì•„ë˜ì˜ í˜•ì‹ìœ¼ë¡œ ì •ë‹µì„ ê¸°ì¬í•´ì£¼ì„¸ìš”.
{{"ì •ë‹µ": "ë²ˆí˜¸"}}"""


system_msg_3 = """You are a student tasked with solving multiple-choice questions. A problem consisting of a passage and a question will be provided.
Please perform reasoning by explicitly detailing your thought process step-by-step to solve the problem. The process leading to the correct answer must be written very logically.
When the solution process is finished, please write the correct answer on the very last line in the following format:
{{"ì •ë‹µ": "number"}}"""


system_msg_4 = """You are a student tasked with solving multiple-choice questions. The problem consists of a passage and a question.
Write extremely clear and evidence-based reasoning according to the instructions below. You must explicitly reveal any lack of background knowledge or logical leaps in your reasoning process. Also, if there is reasoning based on insufficient explanation or incorrect evidence, you must include a process of pointing this out and correcting it yourself.

Instructions:
1. Problem Analysis:
  - First, define what the question is asking for (the core requirement).
  - If background knowledge is needed, describe what knowledge is required and state whether that knowledge is presented in the current passage/problem.
2. Chain of Thought (CoT):
  - Clearly explain why each choice is correct or incorrect by finding evidence in the passage.
  - Do not just find the correct answer; logically explain why the other choices cannot be the answer (process of elimination).
  - Honestly describe points where there are logical leaps or a lack of knowledge-based evidence.
3. Submission Format Compliance:
  - Write the solution process in prose, avoiding unnecessary repetition.
  - Submit the answer on the very last line in the following format:
{{"ì •ë‹µ": "number"}}"""



# =========================
# [NEW] Answer-only variants
# =========================

system_msg_1_1 = """ë‹¹ì‹ ì€ ê°ê´€ì‹ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” í•™ìƒì…ë‹ˆë‹¤. ì•„ë˜ì— ìœ ì‚¬í•œ ë¬¸ì œì˜ ì˜ˆì‹œê°€ ì œê³µë©ë‹ˆë‹¤.

## ì˜ˆì‹œ ë¬¸ì œë“¤

{few_shot_examples}

---

## ì´ì œ ë‹¤ìŒ ë¬¸ì œë¥¼ í’€ì–´ì£¼ì„¸ìš”

ì œì‹œë¬¸ê³¼ ì§ˆë¬¸ìœ¼ë¡œ ì´ë£¨ì–´ì§„ ë¬¸ì œê°€ ì£¼ì–´ì§‘ë‹ˆë‹¤.
ìœ„ ì˜ˆì‹œë“¤ì„ ì°¸ê³ í•˜ì—¬ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ ì‚¬ê³ ê³¼ì •ì„ step-by-stepìœ¼ë¡œ ì¶”ë¡ ì„ ìˆ˜í–‰í•´ì£¼ì„¸ìš”.
ì •ë‹µì— ì´ë¥´ëŠ” ê³¼ì •ì€ ë§¤ìš° ë…¼ë¦¬ì ìœ¼ë¡œ ì‘ì„±ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.

ì¤‘ìš” ê·œì¹™:
- ë¬¸ì œë¥¼ í’€ê¸° ìœ„í•´ í•„ìš”í•œ ì¶”ë¡ ì€ ì¶©ë¶„íˆ ìˆ˜í–‰í•˜ë˜, í’€ì´ê³¼ì •/ì„¤ëª…/ê·¼ê±°ëŠ” ì ˆëŒ€ ì¶œë ¥í•˜ì§€ ë§ˆì„¸ìš”.
- ì¶œë ¥ì€ ì˜¤ì§ ë§ˆì§€ë§‰ í•œ ì¤„ë§Œ í—ˆìš©ë©ë‹ˆë‹¤.
- ë§ˆì§€ë§‰ í•œ ì¤„ì€ ë°˜ë“œì‹œ ì•„ë˜ JSON í˜•ì‹ ê·¸ëŒ€ë¡œì—¬ì•¼ í•©ë‹ˆë‹¤. ë‹¤ë¥¸ ê¸€ì, ê³µë°±, ì¤„ë°”ê¿ˆì„ ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”.

ì¶œë ¥ í˜•ì‹(ë§ˆì§€ë§‰ í•œ ì¤„):
{{"ì •ë‹µ": "ë²ˆí˜¸"}}

ì£¼ì˜:
- ë²ˆí˜¸ëŠ” 1~5 ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤.
- ìœ„ í˜•ì‹ì„ ì–´ê¸°ë©´ ì˜¤ë‹µ ì²˜ë¦¬ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
"""


system_msg_4_1 = """You are a student tasked with solving multiple-choice questions. The problem consists of a passage and a question.

You must fully reason internally using the rules below, but you must NOT output any reasoning.

Internal rules (do not print):
- Identify what the question is asking.
- Check whether required knowledge is provided in the passage.
- Evaluate every choice using evidence from the passage.
- Use elimination to reject unsupported or incorrect choices.
- If evidence is weak, correct your reasoning internally before deciding.

Output rules:
- Output ONLY one line.
- The line must be exactly in the following format:
{{"ì •ë‹µ": "number"}}
- number must be one of: 1, 2, 3, 4, 5
- Do not add any other text.
"""


# Few-shot ì‚¬ìš©
list_system_msg = [system_msg_1_1, system_msg_4_1]


async def get_inference(system_msg, user_content, seed, max_retries=2):
    """ë‹¨ì¼ ì¶”ë¡  ìš”ì²­ ìˆ˜í–‰ (ì„œë²„ ì˜¤ë¥˜ ì‹œ ì¬ì‹œë„ í¬í•¨)"""
    for attempt in range(max_retries + 1):
        try:
            response = await client.chat.completions.create(
                model="Qwen3-Next-80B-A3B-Thinking",
                messages=[
                    {"role": "system", "content": system_msg},
                    {"role": "user", "content": user_content}
                ],
                max_tokens=MAX_TOKENS,
                temperature=1.0,
                top_p=1.0,
                seed=seed,
            )
            msg = response.choices[0].message

            # Answer-only ëª¨ë“œ: ì •ë‹µ JSONë§Œ ë°˜í™˜ (ëª¨ë¸ì€ ë‚´ë¶€ì ìœ¼ë¡œ ì¶”ë¡  ìˆ˜í–‰)
            ret = msg.content
            return ret

        except Exception as e:
            if attempt < max_retries:
                print(f"Inference {seed} Error (attempt {attempt+1}/{max_retries+1}): {e}, retrying...")
                await asyncio.sleep(2)  # 2ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„
            else:
                print(f"Inference {seed} Error (all {max_retries+1} attempts failed): {e}")
                return "Error"


async def process_row(index, row, seed_base, system_msg_template):
    """ë‹¨ì¼ í–‰(ë¬¸ì œ)ì— ëŒ€í•´ few-shot ì¶”ë¡  ìˆ˜í–‰ (í† í° ê¸¸ì´ ì œí•œ í¬í•¨)"""
    problem = ast.literal_eval(row['problems'])

    # Few-shot examples êµ¬ì„±
    few_shot_examples = ""
    few_shot_tokens = 0
    max_fewshot_tokens = int(MAX_INPUT_TOKENS * FEWSHOT_RATIO)  # ì…ë ¥ì˜ 60%ë¥¼ few-shotì— í• ë‹¹

    for shot_num in [1, 2, 3]:
        shot_key = f'shot{shot_num}'
        if pd.notna(row.get(shot_key)):
            shot_text = row[shot_key]
            shot_tokens = estimate_tokens(shot_text)

            # í† í° ì´ˆê³¼ ì‹œ ìë¥´ê¸°
            if few_shot_tokens + shot_tokens > max_fewshot_tokens:
                remaining_tokens = max_fewshot_tokens - few_shot_tokens
                if remaining_tokens > 100:  # ìµœì†Œ 100 í† í°ì€ ë‚¨ê²¨ì•¼ ì˜ë¯¸ ìˆìŒ
                    shot_text = truncate_text(shot_text, remaining_tokens * 4)
                    few_shot_examples += f"### ì˜ˆì‹œ {shot_num}\n{shot_text}\n\n"

                    # Truncation ê²½ê³  ë¡œê·¸ ì¶”ê°€
                    truncation_warnings.append({
                        'row_index': index,
                        'type': 'few_shot',
                        'shot_num': shot_num,
                        'original_tokens': shot_tokens,
                        'remaining_tokens': remaining_tokens,
                        'message': f"Shot {shot_num} truncated"
                    })
                    print(f"Warning: Shot {shot_num} truncated for row {index}")
                break

            few_shot_examples += f"### ì˜ˆì‹œ {shot_num}\n{shot_text}\n\n"
            few_shot_tokens += shot_tokens

    # System messageì— few-shot ì‚½ì…
    system_msg_with_fewshot = system_msg_template.format(few_shot_examples=few_shot_examples)
    system_tokens = estimate_tokens(system_msg_with_fewshot)

    # User content êµ¬ì„±
    paragraph_text = row['paragraph']
    user_content = f"<ì œì‹œë¬¸>\n{paragraph_text}\n\n"
    if pd.notna(row['question_plus']):
        user_content += f"<ë³´ê¸°>\n{row['question_plus']}\n\n"
    user_content += f"<ì§ˆë¬¸>\n{problem['question']}\n"
    for k in range(len(problem['choices'])):
        user_content += f"{k+1}. {problem['choices'][k]}\n"

    user_tokens = estimate_tokens(user_content)

    # ì „ì²´ ì…ë ¥ í† í° ì²´í¬
    total_input_tokens = system_tokens + user_tokens

    if total_input_tokens > MAX_INPUT_TOKENS:
        print(f"Warning: Row {index} exceeds token limit!")
        print(f"  System: {system_tokens}, User: {user_tokens}, Total: {total_input_tokens} > {MAX_INPUT_TOKENS}")

        # User contentë¥¼ ì¤„ì´ê¸° (ì œì‹œë¬¸ë§Œ ìë¥´ê¸°)
        max_paragraph_chars = (MAX_INPUT_TOKENS - system_tokens - 500) * 4  # 500 í† í° ì—¬ìœ 
        original_paragraph_tokens = estimate_tokens(paragraph_text)
        paragraph_text = truncate_text(paragraph_text, max_paragraph_chars)

        # Truncation ê²½ê³  ë¡œê·¸ ì¶”ê°€
        truncation_warnings.append({
            'row_index': index,
            'type': 'paragraph',
            'shot_num': None,
            'original_tokens': original_paragraph_tokens,
            'remaining_tokens': max_paragraph_chars // 4,
            'message': f"Paragraph truncated (System: {system_tokens}, User: {user_tokens})"
        })

        # User content ì¬êµ¬ì„±
        user_content = f"<ì œì‹œë¬¸>\n{paragraph_text}\n\n"
        if pd.notna(row['question_plus']):
            user_content += f"<ë³´ê¸°>\n{row['question_plus']}\n\n"
        user_content += f"<ì§ˆë¬¸>\n{problem['question']}\n"
        for k in range(len(problem['choices'])):
            user_content += f"{k+1}. {problem['choices'][k]}\n"

    seed = (seed_base * len(df_test) + index)

    # Few-shot system message ì‚¬ìš©
    result = await get_inference(system_msg_with_fewshot, user_content, seed)

    # ìˆ˜ì •3: ê²°ê³¼ì— ì •ë‹µì´ ì—†ìœ¼ë©´ ìµœëŒ€ 1íšŒ ì¬ì‹œë„ (submissionê³¼ ë™ì¼í•œ regex ì‚¬ìš©)
    match = re.search(r'ì •ë‹µ.*?["\']\s*(\d)\s*["\']', result)
    if not match or match.group(1) not in ['1', '2', '3', '4', '5']:
        print(f"Warning: Row {index} missing answer format, retrying once...")
        result = await get_inference(system_msg_with_fewshot, user_content, seed + 999999)  # ë‹¤ë¥¸ seedë¡œ ì¬ì‹œë„

    return index, result


async def main():
    """ë©”ì¸ ì¶”ë¡  ë¡œì§"""
    print("Start Inference with Model: Qwen3-Next-80B-A3B-Thinking (Few-shot)")
    print(f"Token Budget: CTX={CTX_SIZE}, MAX_OUTPUT={MAX_TOKENS}, MAX_INPUT={MAX_INPUT_TOKENS}")
    print(f"Ensemble: {NUM_LOOPS} loops Ã— {len(list_system_msg)} system messages = {NUM_LOOPS * len(list_system_msg)} total inferences per row")
    print(f"System Messages: {len(list_system_msg)} templates")

    inference_count = 0
    for s in range(NUM_LOOPS):
        for msg_idx, system_msg_template in enumerate(list_system_msg):
            print(f"\n==== Processing Loop {s+1}/{NUM_LOOPS}, System Msg {msg_idx+1}/{len(list_system_msg)} (inference #{inference_count+1}/{NUM_LOOPS * len(list_system_msg)}) ====")

            # tqdm progress barë¡œ ì§„í–‰ìƒí™© í‘œì‹œ
            for i in tqdm(range(len(df_test)), desc=f"Loop {s+1}-{msg_idx+1}", unit="problem"):
                idx, result = await process_row(i, df_test.iloc[i], s * len(list_system_msg) + msg_idx, system_msg_template)

                # ê²°ê³¼ ì €ì¥
                df_test.loc[idx, f'resp_fewshot_{inference_count}'] = result

                if i % 5 == 4:
                    df_test.to_csv('TestSet_Fewshot.csv', index=False)

            df_test.to_csv('TestSet_Fewshot.csv', index=False)

            inference_count += 1

    # Truncation ê²½ê³  CSV ì €ì¥
    if truncation_warnings:
        df_warnings = pd.DataFrame(truncation_warnings)
        df_warnings.to_csv('truncation_warnings.csv', index=False)
        print(f"\nâš ï¸ Truncation warnings saved to truncation_warnings.csv")
        print(f"   Total truncated rows: {len(df_warnings)}")
        print(f"   - Few-shot truncations: {len(df_warnings[df_warnings['type']=='few_shot'])}")
        print(f"   - Paragraph truncations: {len(df_warnings[df_warnings['type']=='paragraph'])}")
    else:
        print("\nâœ… No truncation occurred!")


def create_submission():
    """ì¶”ë¡  ê²°ê³¼ë¥¼ ì•™ìƒë¸”í•˜ì—¬ ìµœì¢… ì œì¶œ íŒŒì¼ ìƒì„±"""
    # data í´ë” ìƒì„± (ì—†ìœ¼ë©´)
    os.makedirs('./data', exist_ok=True)

    # ì¶”ë¡  ê²°ê³¼ ë¡œë“œ
    df_test = pd.read_csv('TestSet_Fewshot.csv')

    # ìˆ˜ì •1: output.csvë¥¼ testì˜ idë¡œ ìƒì„±/ì¬êµ¬ì„±
    test_ids = df_test['id'].tolist() if 'id' in df_test.columns else list(range(len(df_test)))

    # ê¸°ì¡´ output.csvê°€ ìˆë”ë¼ë„ test idì™€ ë‹¤ë¥´ë©´ ì¬êµ¬ì„±
    if os.path.exists('./data/output.csv'):
        df_output_old = pd.read_csv('./data/output.csv')
        if not df_output_old['id'].equals(pd.Series(test_ids)):
            print("âš ï¸ output.csv id mismatch with test. Reconstructing with test ids...")
            df_output = pd.DataFrame({'id': test_ids, 'answer': ['1']*len(test_ids)})
        else:
            df_output = df_output_old
    else:
        print("âš ï¸ output.csv not found. Creating with test ids...")
        df_output = pd.DataFrame({'id': test_ids, 'answer': ['1']*len(test_ids)})

    # ë‹¤ì–‘ì„± ì²´í¬ìš©
    diversity_stats = []

    # ì „ì²´ inference íšŸìˆ˜ ê³„ì‚°
    total_inferences = NUM_LOOPS * 2  # list_system_msgëŠ” í•­ìƒ 2ê°œ

    for i in range(len(df_test)):
        list_choice = []

        # Few-shot ê²°ê³¼ ì „ë¶€ ì‚¬ìš© (NUM_LOOPS Ã— 2)
        for inf_idx in range(total_inferences):
            try:
                resp = df_test.loc[i, f'resp_fewshot_{inf_idx}']

                # ê³µë°± ì œê±°í•˜ê³  "ì •ë‹µ"ê³¼ ìˆ«ìë§Œ ì¶”ì¶œ
                # {"ì •ë‹µ": "4"}, {" ì •ë‹µ ": "4 "}, {"ì •ë‹µ":"4"} ë“± ëª¨ë‘ ì²˜ë¦¬
                match = re.search(r'ì •ë‹µ.*?["\']\s*(\d)\s*["\']', resp)
                if match:
                    choice = match.group(1)
                    if choice in ['1', '2', '3', '4', '5']:
                        list_choice.append(choice)
            except:
                pass

        # ë‹¤ì–‘ì„± ì²´í¬: ê°™ì€ ë‹µë§Œ ë°˜ë³µë˜ëŠ”ì§€ í™•ì¸
        unique_choices = len(set(list_choice))
        diversity_stats.append({
            'row_index': i,
            'total_responses': len(list_choice),
            'unique_choices': unique_choices,
            'choices': list_choice
        })

        list_choice.sort()
        count_choices = Counter(list_choice)
        top2 = count_choices.most_common(2)

        try:
            df_output.loc[i, 'answer'] = top2[0][0]
        except:
            df_output.loc[i, 'answer'] = '1'

    # ë‹¤ì–‘ì„± í†µê³„ ì €ì¥ (data í´ë” ì•ˆì—)
    df_diversity = pd.DataFrame(diversity_stats)
    df_diversity.to_csv('./data/ensemble_diversity_check.csv', index=False)

    # ë‹¤ì–‘ì„± ìš”ì•½ ì¶œë ¥
    all_same_count = len(df_diversity[df_diversity['unique_choices'] == 1])
    print(f"\nğŸ“Š Ensemble Diversity Check:")
    print(f"   Total problems: {len(df_diversity)}")
    print(f"   All same answer (no diversity): {all_same_count} ({all_same_count/len(df_diversity)*100:.1f}%)")
    print(f"   Has diversity: {len(df_diversity) - all_same_count} ({(len(df_diversity)-all_same_count)/len(df_diversity)*100:.1f}%)")
    print(f"   Average unique choices: {df_diversity['unique_choices'].mean():.2f}")
    print(f"   Diversity stats saved to: ./data/ensemble_diversity_check.csv")

    # ìµœì¢… ì œì¶œ íŒŒì¼ ì €ì¥ (data í´ë” ì•ˆì—)
    df_output.to_csv('./data/submission_fewshot.csv', index=False)
    print(f"\nâœ… Submission file created: ./data/submission_fewshot.csv")


if __name__ == "__main__":
    # ì¶”ë¡  ì‹¤í–‰
    time.sleep(5)
    asyncio.run(main())
    time.sleep(5)

    # ì œì¶œ íŒŒì¼ ìƒì„±
    create_submission()
