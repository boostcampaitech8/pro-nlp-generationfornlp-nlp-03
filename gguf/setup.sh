#!/bin/bash
set -e

echo "ğŸš€ [Native C++] NLP ì¶”ë¡  ì „ìš© í™˜ê²½ êµ¬ì¶• (Only /data/ephemeral)..."


echo "[0/5] ì‘ì—… ê³µê°„ ë° ìºì‹œ ê²½ë¡œ ì„¤ì •..."

WORK_DIR="/data/ephemeral/nlp_workspace"
mkdir -p "$WORK_DIR"
chmod 777 "$WORK_DIR"

export TMPDIR="/data/ephemeral/tmp"
mkdir -p "$TMPDIR"
export TEMP="$TMPDIR"
export TMP="$TMPDIR"

export XDG_CACHE_HOME="/data/ephemeral/.cache"
export PIP_CACHE_DIR="/data/ephemeral/.cache/pip"
export UV_CACHE_DIR="/data/ephemeral/.cache/uv"
export HF_HOME="/data/ephemeral/.cache/huggingface"

mkdir -p "$XDG_CACHE_HOME" "$PIP_CACHE_DIR" "$UV_CACHE_DIR" "$HF_HOME"

cd "$WORK_DIR"
echo "ğŸ“‚ í˜„ì¬ ì‘ì—… ìœ„ì¹˜: $(pwd)"

export DEBIAN_FRONTEND=noninteractive
export TZ=Asia/Seoul
ln -snf /usr/share/zoneinfo/$TZ /etc/localtime


echo "ğŸ“¦ [1/5] ì‹œìŠ¤í…œ íŒ¨í‚¤ì§€ ì„¤ì¹˜ (Build tools)..."
apt-get update
apt-get install -y vim wget build-essential cmake git curl

apt-get clean
rm -rf /var/cache/apt/archives/*


echo "ğŸ“¦ [2/5] CUDA 12.2 í™•ì¸ ë° ì„¤ì¹˜..."

CUDA_RUN="/data/ephemeral/cuda_installer.run"

if [ ! -d "/usr/local/cuda-12.2" ]; then
    echo "â¬‡ï¸ CUDA ë‹¤ìš´ë¡œë“œ..."
    wget -O "$CUDA_RUN" https://developer.download.nvidia.com/compute/cuda/12.2.0/local_installers/cuda_12.2.0_535.54.03_linux.run
    chmod +x "$CUDA_RUN"
    
    echo "âš™ï¸ CUDA ì„¤ì¹˜ ì¤‘..."
    sh "$CUDA_RUN" --silent --toolkit
    
    ln -sf /usr/local/cuda-12.2 /usr/local/cuda
    rm -f "$CUDA_RUN"
    echo "âœ… CUDA ì„¤ì¹˜ ì™„ë£Œ"
else
    echo "âœ… CUDAê°€ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤."
fi

export PATH=/usr/local/cuda/bin:$PATH
export LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH
export CUDACXX=/usr/local/cuda/bin/nvcc


echo "ğŸ [3/5] Python Client í™˜ê²½ êµ¬ì¶•..."

pip install uv
uv init --python 3.12.12 . 
uv sync

source .venv/bin/activate

echo "ğŸ”¥ Python ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ (OpenAI, Pandas)..."
uv pip install --no-cache-dir \
    openai \
    pandas \
    huggingface_hub \
    jupyter \
    ipykernel


echo "ğŸ”¨ [4/5] Llama.cpp ì›ë³¸ ì—”ì§„ ë¹Œë“œ (GPU ê°€ì†)..."

if [ ! -d "llama.cpp" ]; then
    git clone https://github.com/ggerganov/llama.cpp
fi

cd llama.cpp

rm -rf build

cmake -B build -DGGML_CUDA=ON
cmake --build build --config Release -j 6

echo "âœ… ë¹Œë“œ ì™„ë£Œ: $(pwd)/build/bin/llama-server ìƒì„±ë¨"
cd ..


echo "ğŸ’¾ [5/5] ëª¨ë¸ ì €ì¥ì†Œ ì¤€ë¹„..."
mkdir -p models


echo "ğŸ‰ [ì„¤ì¹˜ ì™„ë£Œ]"
