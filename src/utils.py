from ast import literal_eval
import os
import torch
import numpy as np
import random
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import evaluate

from typing import Dict
from datasets import Dataset

sns.set_theme(style="whitegrid")

plt.rcParams["font.family"] = "NanumGothic"
plt.rcParams["axes.unicode_minus"] = False
plt.rcParams["figure.dpi"] = 120

# =============================================================================
# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# =============================================================================


def set_seed(seed: int):
    """ì¬í˜„ì„±ì„ ìœ„í•œ ì‹œë“œ ê³ ì •"""
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    np.random.seed(seed)
    random.seed(seed)
    print(f"âœ“ Seed ê³ ì •: {seed}")


def get_torch_dtype(dtype_str: str):
    """ë¬¸ìì—´ì„ torch dtypeìœ¼ë¡œ ë³€í™˜"""
    dtype_map = {
        "float16": torch.float16,
        "bfloat16": torch.bfloat16,
        "float32": torch.float32,
    }
    return dtype_map.get(dtype_str, torch.float16)


def parse_choices(x):
    if isinstance(x, list):
        return x

    if isinstance(x, str):
        s = x.strip()
        if s.startswith("[") and s.endswith("]"):
            try:
                return literal_eval(s)
            except Exception:
                # ê¹¨ì§„ ë¦¬ìŠ¤íŠ¸ ë¬¸ìì—´
                return []

    return []


def get_token_statistics(dataset: Dataset, tokenizer) -> Dict:
    """í† í° ê¸¸ì´ í†µê³„ ê³„ì‚°"""
    lengths = [len(dataset[i]["input_ids"]) for i in range(len(dataset))]

    return {
        "max": max(lengths),
        "min": min(lengths),
        "mean": sum(lengths) / len(lengths),
        "count": len(lengths),
    }


# =============================================================================
# ì„±ëŠ¥ í‰ê°€ í•¨ìˆ˜
# =============================================================================


def create_metric_functions(tokenizer):

    int_output_map = {"1": 0, "2": 1, "3": 2, "4": 3, "5": 4}

    f1_macro = evaluate.load("f1")
    acc = evaluate.load("accuracy")

    """ë©”íŠ¸ë¦­ ê³„ì‚° í•¨ìˆ˜ë“¤ ìƒì„±"""

    def preprocess_logits_for_metrics(logits, labels):
        """ì •ë‹µ í† í° ìœ„ì¹˜ì˜ logitsë§Œ ì¶”ì¶œ"""
        logits = logits if not isinstance(logits, tuple) else logits[0]
        logit_idx = [
            tokenizer.vocab["1"],
            tokenizer.vocab["2"],
            tokenizer.vocab["3"],
            tokenizer.vocab["4"],
            tokenizer.vocab["5"],
        ]
        logits = logits[:, -2, logit_idx]  # -2: answer token, -1: eos token
        return logits

    def compute_metrics(evaluation_result):
        """ì •í™•ë„ ê³„ì‚°"""
        logits, labels = evaluation_result

        # í† í°í™”ëœ ë ˆì´ë¸” ë””ì½”ë”©
        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
        labels = tokenizer.batch_decode(labels, skip_special_tokens=True)
        labels = list(map(lambda x: x.split("<end_of_turn>")[0].strip(), labels))
        labels = list(map(lambda x: int_output_map.get(x, 0), labels))

        # Softmaxë¡œ í™•ë¥  ë³€í™˜
        probs = torch.nn.functional.softmax(torch.tensor(logits), dim=-1)
        predictions = np.argmax(probs, axis=-1)

        # ì •í™•ë„ ê³„ì‚°
        macro_f1 = f1_macro.compute(
            predictions=predictions, references=labels, average="macro"
        )
        acc = acc.compute(predictions=predictions, references=labels)

        return {"macro_f1": macro_f1, "accuracy": acc}

    return preprocess_logits_for_metrics, compute_metrics


# =============================================================================
# ì‹œê°í™” í•¨ìˆ˜
# =============================================================================


def analyze_subject_accuracy(
    df,
    true_col="label",  # ì§„ì§œ ì •ë‹µ
    pred_col="answer",  # ëª¨ë¸ ì˜ˆì¸¡
    topic_col="topic",  # ê³¼ëª© (ì‚¬íšŒ, ê²½ì œ, â€¦)
    save_dir=None,
):
    """
    [ë³´ê³ ìš©]
    ê³¼ëª©ë³„ ì •ë‹µ ê°œìˆ˜ / ì „ì²´ ê°œìˆ˜ / ì •ë‹µ ë¹„ìœ¨ ë¶„ì„

    ê¸°ì¤€:
    - ì •ë‹µ ì—¬ë¶€: true_col == pred_col
    - ê³¼ëª©ë³„ ì§‘ê³„: topic_col
    """

    if save_dir:
        os.makedirs(save_dir, exist_ok=True)

    df = df.copy()

    # -----------------------------
    # ì •ë‹µ ì—¬ë¶€ íŒë‹¨ (í•µì‹¬)
    # -----------------------------
    df["correct"] = df[true_col] == df[pred_col]

    # -----------------------------
    # ê³¼ëª©ë³„ ì§‘ê³„
    # -----------------------------
    result_df = (
        df.groupby(topic_col)
        .agg(total_count=("correct", "size"), correct_count=("correct", "sum"))
        .reset_index()
    )

    result_df["correct_ratio"] = result_df["correct_count"] / result_df["total_count"]

    # ì •ë‹µë¥  ë‚®ì€ ê³¼ëª©ë¶€í„° ì •ë ¬
    result_df = result_df.sort_values("correct_ratio")

    # -----------------------------
    # í‘œ ì¶œë ¥ (ë³´ê³ ìš©)
    # -----------------------------
    print(f"\nğŸ“Š {topic_col}-wise Accuracy Report")
    print(
        result_df.rename(
            columns={
                topic_col: "Subject",
                "total_count": "Total",
                "correct_count": "Correct",
                "correct_ratio": "Accuracy",
            }
        )
    )

    # -----------------------------
    # ì‹œê°í™” (ë³´ê³ ìš©)
    # -----------------------------
    plt.figure(figsize=(11, max(4, len(result_df) * 0.38)))

    colors = [
        "firebrick" if r < 0.3 else "darkorange" if r < 0.6 else "seagreen"
        for r in result_df["correct_ratio"]
    ]

    bars = plt.barh(result_df[topic_col], result_df["correct_ratio"], color=colors)

    for _, row in result_df.iterrows():
        plt.text(
            row["correct_ratio"] + 0.01,
            row[topic_col],
            f"{row['correct_count']} / {row['total_count']}  ({row['correct_ratio']:.2f})",
            va="center",
            fontsize=10,
        )

    plt.xlim(0, 1)
    plt.xlabel("Accuracy (Correct / Total)")
    plt.title(
        f"{topic_col}-wise Accuracy\n(How many questions were answered correctly per {topic_col})",
        fontsize=14,
        weight="bold",
    )

    sns.despine(left=True, bottom=True)
    plt.tight_layout()

    if save_dir:
        os.makedirs(save_dir, exist_ok=True)
        print(f"ğŸ“‚ í´ë” ìƒì„± ì™„ë£Œ: {save_dir}")

    if save_dir:
        plt.savefig(f"{save_dir}/{topic_col}_accuracy.png")

    plt.show()

    return result_df


def balance_answer_by_swap(df):
    """
    Using the 'choice_len' column,
    evenly distribute the correct answers in the data.
    """

    invalid_mask = (df["choice_len"] <= 0) | (df["choice_len"] > 5)

    if invalid_mask.any():
        invalid_count = len(df[invalid_mask])
        print(
            f"âš ï¸ ê²½ê³ : ìœ íš¨í•˜ì§€ ì•Šì€ ì„ íƒì§€ ê°œìˆ˜(0ê°œ ë˜ëŠ” 5ê°œ ì´ˆê³¼)ë¥¼ ê°€ì§„ ë°ì´í„° {invalid_count}ê±´ì„ ì œì™¸í•©ë‹ˆë‹¤."
        )
        # ìœ íš¨í•œ ë²”ìœ„(1~5)ì¸ ë°ì´í„°ë§Œ ë‚¨ê¹€
        df = df[~invalid_mask].reset_index(drop=True)

    total_len = len(df)

    # 1. ì „ì²´ ëª©í‘œ ì •ë‹µ ë¦¬ìŠ¤íŠ¸ ìƒì„± (1~5ë²ˆì´ ê° 812ê°œì”©)
    all_targets = ([1, 2, 3, 4, 5] * (total_len // 5 + 1))[:total_len]
    np.random.seed(42)
    np.random.shuffle(all_targets)

    # ì œì•½ ì¡°ê±´ í•´ê²° (4ì§€ì„ ë‹¤ëŠ” 5ë²ˆì„ ê°€ì§ˆ ìˆ˜ ì—†ìŒ)
    # ëª©í‘œ ì •ë‹µì´ 5ì¸ë° ë¬¸í•­ì´ 4ì§€ì„ ë‹¤ì¸ ê²½ìš°, 5ì§€ì„ ë‹¤ ë¬¸í•­ì˜ 1~4ë²ˆ ì •ë‹µê³¼ ë§ë°”ê¿ˆ
    df_4 = df[df["choice_len"] == 4].index.tolist()
    df_5 = df[df["choice_len"] == 5].index.tolist()

    # 5ë²ˆ ì •ë‹µì´ ë°°ì •ëœ ì¸ë±ìŠ¤ë“¤
    target_5_indices = [i for i, val in enumerate(all_targets) if val == 5]

    for idx in target_5_indices:
        # ë§Œì•½ 5ë²ˆ ì •ë‹µì´ ë°°ì •ëœ ê³³ì´ 4ì§€ì„ ë‹¤ ë¬¸í•­ì´ë¼ë©´?
        if idx in df_4:
            # 5ì§€ì„ ë‹¤ ë¬¸í•­ ì¤‘ ì •ë‹µì´ 1~4ë²ˆìœ¼ë¡œ ë°°ì •ëœ ì•„ë¬´ ì¸ë±ìŠ¤ë‚˜ ì°¾ì•„ì„œ êµì²´
            for swap_idx in range(total_len):
                if swap_idx in df_5 and all_targets[swap_idx] != 5:
                    all_targets[idx], all_targets[swap_idx] = (
                        all_targets[swap_idx],
                        all_targets[idx],
                    )
                    break

    # ê²°ì •ëœ ì •ë‹µ(all_targets)ì— ë§ì¶° ìŠ¤ì™‘ ì‹¤í–‰
    final_choices = []
    final_answers = []

    for idx, row in df.iterrows():
        current_choices = list(row["choices"])
        target_ans = all_targets[idx]

        current_ans_idx = int(row["answer"]) - 1
        target_ans_idx = target_ans - 1

        # ì‹¤ì œ ë¦¬ìŠ¤íŠ¸ ë‚´ í…ìŠ¤íŠ¸ ìœ„ì¹˜ êµì²´
        current_choices[current_ans_idx], current_choices[target_ans_idx] = (
            current_choices[target_ans_idx],
            current_choices[current_ans_idx],
        )

        final_choices.append(current_choices)
        final_answers.append(target_ans)

    df["choices"] = final_choices
    df["answer"] = final_answers

    return df


# -----------------------------
# ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ
# -----------------------------
def get_latest_checkpoint(checkpoint_dir):
    if not os.path.isdir(checkpoint_dir):
        return None

    checkpoints = [
        os.path.join(checkpoint_dir, d)
        for d in os.listdir(checkpoint_dir)
        if d.startswith("checkpoint-")
    ]

    if not checkpoints:
        return None

    return max(checkpoints, key=lambda x: int(x.split("-")[-1]))
