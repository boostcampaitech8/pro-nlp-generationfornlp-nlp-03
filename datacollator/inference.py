"""
Inference ìŠ¤í¬ë¦½íŠ¸
Usage:
    # Valid setìœ¼ë¡œ F1/Accuracy ê³„ì‚°
    python inference.py --mode valid --checkpoint ./results/checkpoint-best

    # Test setìœ¼ë¡œ submission.csv ìƒì„±
    python inference.py --mode test --checkpoint ./results/checkpoint-best
"""

import os
import argparse
import torch
import numpy as np
import pandas as pd
from tqdm import tqdm
from typing import List, Dict

from unsloth import FastLanguageModel
from transformers import AutoTokenizer
from sklearn.metrics import f1_score, accuracy_score
from peft import PeftModel  # âœ… ì¶”ê°€

from config import get_config
from data_utils import (
    load_data,
    process_dataset_for_inference,
    setup_tokenizer,
)


def get_answer_token_ids(tokenizer):
    """1~5 í† í° ID ì¶”ì¶œ"""
    choice_token_ids = []
    for s in ["1", "2", "3", "4", "5"]:
        tid = tokenizer.encode(s, add_special_tokens=False)
        if len(tid) != 1:
            raise ValueError(f"í† í°í™” ì˜ˆìƒê³¼ ë‹¤ë¦„: '{s}' -> {tid}")
        choice_token_ids.append(tid[0])

    print(f"âœ“ ì •ë‹µ í† í° ID: {dict(zip(['1','2','3','4','5'], choice_token_ids))}")
    return choice_token_ids


def inference_with_logits(
    model,
    tokenizer,
    input_ids,
    attention_mask,
    choice_token_ids,
):
    """
    Logits ê¸°ë°˜ ì¶”ë¡  (train.py metricê³¼ ë™ì¼ ë°©ì‹)

    Returns:
        predicted answer (0~4)
    """
    with torch.no_grad():
        outputs = model(
            input_ids=input_ids,
            attention_mask=attention_mask,
        )
        logits = outputs.logits  # (batch, seq_len, vocab_size)

    # âœ… assistant ë‹µë³€ ì‹œì‘ ìœ„ì¹˜ ì°¾ê¸°
    # <|im_start|>assistant ë‹¤ìŒ í† í°ì´ ì •ë‹µ (1~5)
    response_template = "<|im_start|>assistant"
    response_ids = tokenizer.encode(response_template, add_special_tokens=False)

    batch_size = input_ids.size(0)
    predictions = []

    for i in range(batch_size):
        ids_list = input_ids[i].tolist()

        # response_template ìœ„ì¹˜ ì°¾ê¸°
        start = _find_sublist(ids_list, response_ids)
        if start == -1:
            # ëª» ì°¾ìœ¼ë©´ ë§ˆì§€ë§‰ í† í° ì‚¬ìš©
            ans_pos = len(ids_list) - 1
        else:
            # response_template ë ë‹¤ìŒ = ì •ë‹µ í† í° ìœ„ì¹˜
            ans_pos = start + len(response_ids)

        # í•´ë‹¹ ìœ„ì¹˜ì˜ logitsì—ì„œ 1~5 í† í°ë§Œ ì¶”ì¶œ
        ans_logits = logits[i, ans_pos, choice_token_ids]  # (5,)
        pred = torch.argmax(ans_logits).item()  # 0~4
        predictions.append(pred)

    return predictions


def _find_sublist(haystack, needle):
    """ë¦¬ìŠ¤íŠ¸ì—ì„œ ì„œë¸Œë¦¬ìŠ¤íŠ¸ ì°¾ê¸°"""
    n = len(needle)
    if n == 0:
        return -1
    for i in range(len(haystack) - n + 1):
        if haystack[i:i+n] == needle:
            return i
    return -1


def run_inference_valid(config, checkpoint_path):
    """Valid setìœ¼ë¡œ F1/Accuracy ê³„ì‚°"""

    print("=" * 60)
    print("ğŸ“Š Valid Set Inference (F1 + Accuracy)")
    print("=" * 60)

    # 1. ëª¨ë¸/í† í¬ë‚˜ì´ì € ë¡œë“œ
    print(f"\nğŸ¤– ëª¨ë¸ ë¡œë“œ ì¤‘...")
    print(f"  - Base model: {config.model.model_name}")
    print(f"  - Adapter: {checkpoint_path}")

    # âœ… Base model ë¨¼ì € ë¡œë“œ
    model, tokenizer = FastLanguageModel.from_pretrained(
        config.model.model_name,  # âœ… Base model
        dtype=torch.float16,
        load_in_4bit=True,
        max_seq_length=config.training.max_seq_length,
    )

    # âœ… Adapter ë¡œë“œ
    # from peft import PeftModel  # Already imported at top
    model = PeftModel.from_pretrained(model, checkpoint_path)

    model.eval()
    tokenizer = setup_tokenizer(tokenizer)

    print("  âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")

    choice_token_ids = get_answer_token_ids(tokenizer)

    # 2. Valid ë°ì´í„° ë¡œë“œ
    print(f"\nğŸ“‚ Valid ë°ì´í„° ë¡œë“œ: {config.path.valid_data}")
    df_valid = load_data(config.path.valid_data)
    valid_data = process_dataset_for_inference(df_valid)
    print(f"  - Valid samples: {len(valid_data)}")

    # âœ… Label ì²´í¬
    none_labels = [d['id'] for d in valid_data if d['label'] is None]
    if none_labels:
        print(f"\nâš ï¸ ê²½ê³ : {len(none_labels)}ê°œ ìƒ˜í”Œì— answerê°€ ì—†ìŠµë‹ˆë‹¤!")
        print(f"  ì²˜ìŒ 5ê°œ: {none_labels[:5]}")
        print(f"  â†’ Valid setì€ answerê°€ í•„ìš”í•©ë‹ˆë‹¤. ë°ì´í„°ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        # None ì œê±°
        valid_data = [d for d in valid_data if d['label'] is not None]
        print(f"  â†’ í•„í„°ë§ í›„: {len(valid_data)} samples")

    # 3. Inference
    print("\nğŸ”® Inference ì‹¤í–‰ ì¤‘...")
    all_predictions = []
    all_labels = []

    device = model.device
    batch_size = config.training.per_device_eval_batch_size

    for i in tqdm(range(0, len(valid_data), batch_size), desc="Processing"):
        batch_data = valid_data[i:i+batch_size]

        # í† í°í™”
        batch_texts = []
        batch_labels = []
        for item in batch_data:
            # âœ… labelì´ Noneì¸ ê²½ìš° ì²´í¬
            if item["label"] is None:
                print(f"\nâš ï¸ ê²½ê³ : {item['id']} - labelì´ Noneì…ë‹ˆë‹¤. ìŠ¤í‚µí•©ë‹ˆë‹¤.")
                continue

            text = tokenizer.apply_chat_template(
                item["messages"],
                tokenize=False,
                add_generation_prompt=True,
            )
            batch_texts.append(text)
            batch_labels.append(item["label"] - 1)  # 1~5 -> 0~4

        # âœ… batchê°€ ë¹„ì–´ìˆìœ¼ë©´ ìŠ¤í‚µ
        if len(batch_texts) == 0:
            continue

        # ì¸ì½”ë”©
        encoded = tokenizer(
            batch_texts,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=config.training.max_seq_length,
        ).to(device)

        # ì˜ˆì¸¡
        predictions = inference_with_logits(
            model,
            tokenizer,
            encoded["input_ids"],
            encoded["attention_mask"],
            choice_token_ids,
        )

        all_predictions.extend(predictions)
        all_labels.extend(batch_labels)

    # 4. ë©”íŠ¸ë¦­ ê³„ì‚°
    print("\n" + "=" * 60)
    print("âœ… ê²°ê³¼")
    print("=" * 60)

    f1 = f1_score(all_labels, all_predictions, average="macro")
    acc = accuracy_score(all_labels, all_predictions)

    print(f"  - Macro F1:  {f1:.4f}")
    print(f"  - Accuracy:  {acc:.4f}")
    print(f"  - Total samples: {len(all_labels)}")

    # 5. ì˜¤ë‹µ ë¶„ì„ (ì„ íƒ)
    errors = []
    for i, (pred, label) in enumerate(zip(all_predictions, all_labels)):
        if pred != label:
            errors.append({
                'id': valid_data[i]['id'],
                'predicted': pred + 1,
                'actual': label + 1,
            })

    if errors:
        print(f"\nâŒ ì˜¤ë‹µ: {len(errors)}ê°œ")
        print("  ì²˜ìŒ 5ê°œ:")
        for err in errors[:5]:
            print(f"    {err['id']}: ì˜ˆì¸¡={err['predicted']}, ì •ë‹µ={err['actual']}")

    return f1, acc


def run_inference_test(config, checkpoint_path):
    """Test setìœ¼ë¡œ submission.csv ìƒì„±"""

    print("=" * 60)
    print("ğŸ“ Test Set Inference (Submission ìƒì„±)")
    print("=" * 60)

    # 1. ëª¨ë¸/í† í¬ë‚˜ì´ì € ë¡œë“œ
    print(f"\nğŸ¤– ëª¨ë¸ ë¡œë“œ ì¤‘...")
    print(f"  - Base model: {config.model.model_name}")
    print(f"  - Adapter: {checkpoint_path}")

    # âœ… Base model ë¨¼ì € ë¡œë“œ
    model, tokenizer = FastLanguageModel.from_pretrained(
        config.model.model_name,  # âœ… Base model
        dtype=torch.float16,
        load_in_4bit=True,
        max_seq_length=config.training.max_seq_length,
    )

    # âœ… Adapter ë¡œë“œ
    # from peft import PeftModel  # Already imported at top
    model = PeftModel.from_pretrained(model, checkpoint_path)

    model.eval()
    tokenizer = setup_tokenizer(tokenizer)

    print("  âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")

    choice_token_ids = get_answer_token_ids(tokenizer)

    # 2. Test ë°ì´í„° ë¡œë“œ
    print(f"\nğŸ“‚ Test ë°ì´í„° ë¡œë“œ: {config.path.test_data}")
    df_test = load_data(config.path.test_data)
    test_data = process_dataset_for_inference(df_test)
    print(f"  - Test samples: {len(test_data)}")

    # 3. Inference
    print("\nğŸ”® Inference ì‹¤í–‰ ì¤‘...")
    all_predictions = []
    all_ids = []

    device = model.device
    batch_size = config.training.per_device_eval_batch_size

    for i in tqdm(range(0, len(test_data), batch_size), desc="Processing"):
        batch_data = test_data[i:i+batch_size]

        # í† í°í™”
        batch_texts = []
        batch_ids = []
        for item in batch_data:
            text = tokenizer.apply_chat_template(
                item["messages"],
                tokenize=False,
                add_generation_prompt=True,
            )
            batch_texts.append(text)
            batch_ids.append(item["id"])

        # ì¸ì½”ë”©
        encoded = tokenizer(
            batch_texts,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=config.training.max_seq_length,
        ).to(device)

        # ì˜ˆì¸¡
        predictions = inference_with_logits(
            model,
            tokenizer,
            encoded["input_ids"],
            encoded["attention_mask"],
            choice_token_ids,
        )

        # 0~4 -> 1~5ë¡œ ë³€í™˜
        predictions = [p + 1 for p in predictions]

        all_predictions.extend(predictions)
        all_ids.extend(batch_ids)

    # 4. Submission CSV ìƒì„±
    print("\nğŸ’¾ Submission ì €ì¥ ì¤‘...")
    submission_df = pd.DataFrame({
        'id': all_ids,
        'answer': all_predictions,
    })

    # output_csv ê²½ë¡œ ì‚¬ìš©
    output_path = config.path.output_csv
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    submission_df.to_csv(output_path, index=False)

    print(f"  âœ… ì €ì¥ ì™„ë£Œ: {output_path}")
    print(f"  - Total: {len(submission_df)} rows")

    # 5. ìƒ˜í”Œ ì¶œë ¥
    print("\nğŸ“‹ Submission ìƒ˜í”Œ (ì²˜ìŒ 10ê°œ):")
    print(submission_df.head(10).to_string(index=False))

    # 6. ë‹µë³€ ë¶„í¬
    print("\nğŸ“Š ë‹µë³€ ë¶„í¬:")
    for ans in [1, 2, 3, 4, 5]:
        count = (submission_df['answer'] == ans).sum()
        pct = count / len(submission_df) * 100
        print(f"  {ans}: {count} ({pct:.1f}%)")

    return submission_df


def main():
    parser = argparse.ArgumentParser(description="Inference")
    parser.add_argument(
        "--mode",
        type=str,
        required=True,
        choices=["valid", "test"],
        help="valid: F1/Acc ê³„ì‚°, test: submission.csv ìƒì„±"
    )
    parser.add_argument(
        "--checkpoint",
        type=str,
        required=True,
        help="ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ (ì˜ˆ: ./results/checkpoint-best)"
    )
    parser.add_argument(
        "--config_path",
        type=str,
        default=None,
        help="ì»¤ìŠ¤í…€ config íŒŒì¼ ê²½ë¡œ (ì„ íƒ)"
    )

    args = parser.parse_args()

    # Config ë¡œë“œ
    config = get_config()
    print(f"ğŸ“Œ Config ë¡œë“œ ì™„ë£Œ")
    print(f"  - Max seq length: {config.training.max_seq_length}")
    print(f"  - Eval batch size: {config.training.per_device_eval_batch_size}")

    # Inference ì‹¤í–‰
    if args.mode == "valid":
        f1, acc = run_inference_valid(config, args.checkpoint)
        print("\n" + "=" * 60)
        print("ğŸ‰ Valid Inference ì™„ë£Œ!")
        print("=" * 60)

    elif args.mode == "test":
        submission_df = run_inference_test(config, args.checkpoint)
        print("\n" + "=" * 60)
        print("ğŸ‰ Test Inference ì™„ë£Œ!")
        print(f"ğŸ“ Submission: {config.path.output_csv}")
        print("=" * 60)


if __name__ == "__main__":
    main()