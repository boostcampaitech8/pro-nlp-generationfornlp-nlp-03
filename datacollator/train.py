"""
í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
Usage:
    python train.py
    python train.py --exp large_context
"""
import sys
import unsloth
import os
import argparse
import torch
import numpy as np
import random
import json
import evaluate

from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments
from trl import SFTTrainer, SFTConfig
from peft import LoraConfig
from unsloth import FastLanguageModel, is_bfloat16_supported

from torch.utils.data import DataLoader

# from code.config import get_config, get_experiment_config
from config import get_config, get_experiment_config
from data_utils import (
    load_data,
    process_dataset_for_training,
    setup_tokenizer,
    tokenize_dataset,
    get_token_statistics
)


# =============================================================================
# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# =============================================================================
from transformers import DataCollatorWithPadding
from transformers import TrainerCallback

class AddStepToLogsCallback(TrainerCallback):
    def on_log(self, args, state, control, logs=None, **kwargs):
        if logs is None:
            return
        # logs dictì— step ì¶”ê°€
        logs["step"] = state.global_step
        # ë³´ê¸° ì¢‹ê²Œ í•œ ì¤„ ì¶œë ¥(ì›í•˜ë©´ ì‚­ì œ)
        print(logs)

def _find_sublist(lst, sub):
    """lst ì•ˆì—ì„œ subê°€ ì²˜ìŒ ë“±ì¥í•˜ëŠ” ì‹œì‘ ì¸ë±ìŠ¤ ë°˜í™˜, ì—†ìœ¼ë©´ -1"""
    n, m = len(lst), len(sub)
    if m == 0 or n < m:
        return -1
    for i in range(n - m + 1):
        if lst[i:i+m] == sub:
            return i
    return -1

# ============================ CODE REVIEW REQUEST =============================
# Review focus:
# 1) CompletionOnlyDataCollatorê°€ ì •ë‹µ ë²ˆí˜¸ë§Œ ì˜ ë§ˆìŠ¤í‚¹í•´ì„œ loss ê³„ì‚°ì— ë“¤ì–´ê°€ëŠ”ì§€ 
# 2) (ì„¤ëª…ì´ ê°€ëŠ¥í•˜ì‹œë‹¤ë©´) SFTTrainerì— DataCollatorì„ ì•ˆë„£ì–´ë„ ì„±ëŠ¥ ì°¨ì´ê°€ ì—†ëŠ”ë° ì–´ë–¤ ì°¨ì´ì¸ì§€.. 
# ==============================================================================
class CompletionOnlyDataCollator:
    """
    response_template(ê¸°ë³¸: <|im_start|>assistant) ì´í›„ì˜ ë‹µë³€ì—ì„œ
    '1~5' ì •ë‹µ í† í° 1ê°œë§Œ labelsë¡œ ë‚¨ê¸°ê³  ë‚˜ë¨¸ì§€ëŠ” -100 ë§ˆìŠ¤í‚¹.
    => loss/metric ë‘˜ ë‹¤ ì•ˆì •í™”ë¨ (ê°ê´€ì‹ ë¶„ë¥˜ì— ìµœì )
    """
    def __init__(self, tokenizer, response_template="<|im_start|>assistant", ignore_index=-100):
        self.tokenizer = tokenizer
        self.ignore_index = ignore_index
        self.pad_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors="pt")

        # response template ids
        self.response_ids = tokenizer.encode(response_template, add_special_tokens=False)
        if len(self.response_ids) == 0:
            raise ValueError("response_templateì´ í† í°í™”ë˜ì§€ ì•Šì•˜ì–´ìš”. í…œí”Œë¦¿ ë¬¸ìì—´ì„ í™•ì¸í•´ì¤˜!")

        # <|im_end|> í† í° id (ìˆìœ¼ë©´ ë§ˆìŠ¤í‚¹ìš©)
        end_ids = tokenizer.encode("<|im_end|>", add_special_tokens=False)
        self.im_end_id = end_ids[0] if len(end_ids) == 1 else None

        # âœ… 1~5 í† í° id (ë°˜ë“œì‹œ 1í† í°ì´ì–´ì•¼ logits ë°©ì‹ì´ ê¹”ë”í•¨)
        self.choice_token_ids = []
        for s in ["1", "2", "3", "4", "5"]:
            ids = tokenizer.encode(s, add_special_tokens=False)
            if len(ids) != 1:
                raise ValueError(f"'{s}'ê°€ 1 í† í°ì´ ì•„ë‹™ë‹ˆë‹¤: {ids} (tokenizer ë³€ê²½/ê³µë°±/í…œí”Œë¦¿ í™•ì¸ í•„ìš”)")
            self.choice_token_ids.append(ids[0])

        print(f"âœ“ Response template: {response_template}")
        print(f"âœ“ Response template IDs: {self.response_ids}")
        print(f"âœ“ im_end_id: {self.im_end_id}")
        print(f"âœ“ choice_token_ids(1~5): {self.choice_token_ids}")

    def __call__(self, features):
        batch = self.pad_collator(features)
        input_ids = batch["input_ids"]
        attention_mask = batch["attention_mask"]

        labels = input_ids.clone()
        labels[attention_mask == 0] = self.ignore_index  # padëŠ” loss ì œì™¸

        for i in range(input_ids.size(0)):
            ids_list = input_ids[i].tolist()

            start = _find_sublist(ids_list, self.response_ids)
            if start == -1:
                labels[i, :] = self.ignore_index
                continue

            end = start + len(self.response_ids)  # assistant ì‹œì‘ ì§í›„

            # 1) í”„ë¡¬í”„íŠ¸ ì „ì²´ ë§ˆìŠ¤í‚¹
            labels[i, :end] = self.ignore_index

            # 2) assistant ì´í›„ì—ì„œ "1~5"ê°€ ì²˜ìŒ ë“±ì¥í•˜ëŠ” ìœ„ì¹˜ë¥¼ ì°¾ëŠ”ë‹¤
            ans_idx = None
            for j in range(end, len(ids_list)):
                if ids_list[j] in self.choice_token_ids:
                    ans_idx = j
                    break

            if ans_idx is None:
                # ì •ë‹µ ìˆ«ì í† í°ì´ ì—†ìœ¼ë©´ í•™ìŠµ ì‹ í˜¸ ì œê±°
                labels[i, :] = self.ignore_index
                continue

            # 3) âœ… ì •ë‹µ 1í† í°ë§Œ ë‚¨ê¸°ê³  ë‚˜ë¨¸ì§€ëŠ” ì „ë¶€ ë§ˆìŠ¤í‚¹
            labels[i, :ans_idx] = self.ignore_index
            labels[i, ans_idx+1:] = self.ignore_index

            # 4) (ì„ íƒ) im_endëŠ” ë§ˆìŠ¤í‚¹
            if self.im_end_id is not None:
                labels[i, labels[i] == self.im_end_id] = self.ignore_index

        batch["labels"] = labels
        return batch

def set_seed(seed: int):
    """ì¬í˜„ì„±ì„ ìœ„í•œ ì‹œë“œ ê³ ì •"""
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    np.random.seed(seed)
    random.seed(seed)
    print(f"âœ“ Seed ê³ ì •: {seed}")


def get_torch_dtype(dtype_str: str):
    """ë¬¸ìì—´ì„ torch dtypeìœ¼ë¡œ ë³€í™˜"""
    dtype_map = {
        "float16": torch.float16,
        "bfloat16": torch.bfloat16,
        "float32": torch.float32,
    }
    return dtype_map.get(dtype_str, torch.float16)

# ================================
# ì„¤ì •/í•™ìŠµ íŒŒë¼ë¯¸í„° ì¶œë ¥ìš©
# ================================
def pretty_print_config(config):
    """config.pyì—ì„œ ë¶ˆëŸ¬ì˜¨ ì›ë³¸ ì„¤ì •ê°’(ì˜ë„í•œ ê°’) ì¶œë ¥"""
    print("\n" + "=" * 80)
    print("ğŸ“‹ CONFIG (from config.py)")
    print("=" * 80)
    try:
        print(json.dumps(asdict(config), indent=2, ensure_ascii=False))
    except Exception as e:
        print("âŒ config ì¶œë ¥ ì‹¤íŒ¨:", e)
        print(config)
    print("=" * 80 + "\n")


def print_trainer_args(trainer):
    """trainer.argsì— ì‹¤ì œë¡œ ì ìš©ëœ ê°’(ì§„ì§œ ì ìš©ê°’) ì¶œë ¥"""
    print("\n" + "=" * 80)
    print("âš™ï¸ TRAINER ARGS (actual applied)")
    print("=" * 80)

    keys = [
        # ë°°ì¹˜/ìŠ¤í…
        "per_device_train_batch_size", "per_device_eval_batch_size",
        "gradient_accumulation_steps", "num_train_epochs", "max_steps",
        # lr / scheduler
        "learning_rate", "weight_decay", "lr_scheduler_type", "warmup_ratio", "warmup_steps",
        # precision
        "fp16", "bf16",
        # logging / save / eval
        "logging_steps", "save_strategy", "save_steps", "save_total_limit",
        "evaluation_strategy", "eval_strategy", "eval_steps",
        # best model
        "load_best_model_at_end", "metric_for_best_model", "greater_is_better",
        # output
        "output_dir",
    ]

    for k in keys:
        if hasattr(trainer.args, k):
            print(f"{k:>24}: {getattr(trainer.args, k)}")

    print("=" * 80 + "\n")


# =============================================================================
# ë©”íŠ¸ë¦­ í•¨ìˆ˜
# =============================================================================
def create_metric_functions(tokenizer):
    # âœ… 1~5 í† í° id ë§Œë“¤ê¸° (ë°˜ë“œì‹œ 1í† í°ì´ì–´ì•¼ í•¨)
    choice_token_ids = []
    for s in ["1", "2", "3", "4", "5"]:
        ids = tokenizer.encode(s, add_special_tokens=False)
        if len(ids) != 1:
            raise ValueError(f"'{s}' is not 1 token: {ids}")
        choice_token_ids.append(ids[0])

    f1_metric = evaluate.load("f1")

    def preprocess_logits_for_metrics(logits, labels):
        logits = logits if not isinstance(logits, tuple) else logits[0]  # (B, T, V)
        B, T, V = logits.shape

        labels_t = labels
        ans_pos = []
        for i in range(B):
            positions = (labels_t[i] != -100).nonzero(as_tuple=False).squeeze(-1)
            ans_pos.append(int(positions[0].item()) if positions.numel() > 0 else T - 1)
        ans_pos = torch.tensor(ans_pos, device=logits.device)

        batch_idx = torch.arange(B, device=logits.device)
        ans_logits = logits[batch_idx, ans_pos, :]              # (B, V)

        # (B, 5)
        ans_logits_5 = ans_logits[:, choice_token_ids]
        return ans_logits_5

    def compute_metrics(eval_pred):
        logits_5, labels = eval_pred  # logits_5: (B,5), labels: (B,T)

        B, T = labels.shape
        y_true = []
        valid_idx = []

        for i in range(B):
            positions = np.where(labels[i] != -100)[0]
            if len(positions) == 0:
                continue
            tid = int(labels[i, positions[0]])
            if tid in choice_token_ids:
                y_true.append(choice_token_ids.index(tid))  # 0~4
                valid_idx.append(i)

        if len(valid_idx) == 0:
            return {"f1": 0.0}

        logits_5_valid = logits_5[valid_idx]
        y_pred = np.argmax(logits_5_valid, axis=-1)

        # macro f1
        return f1_metric.compute(predictions=y_pred, references=y_true, average="macro")

    # âœ… ì—¬ê¸° returnì´ "ë°˜ë“œì‹œ" í•¨ìˆ˜ ìµœí•˜ë‹¨ì— ìˆì–´ì•¼ í•¨!!
    return preprocess_logits_for_metrics, compute_metrics


# =============================================================================
# ë©”ì¸ í•™ìŠµ í•¨ìˆ˜
# =============================================================================

def train(config):
    """ëª¨ë¸ í•™ìŠµ ì‹¤í–‰"""

    print("=" * 60)
    print("ğŸš€ í•™ìŠµ ì‹œì‘")
    print("=" * 60)

    # 1. ì‹œë“œ ê³ ì •
    set_seed(config.training.seed)
    pretty_print_config(config)

    # 2. ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
    print("\nğŸ“‚ ë°ì´í„° ë¡œë“œ ì¤‘...")
    # âœ… train / valid ë°ì´í„° ë‘˜ ë‹¤ ë¡œë“œ
    df_train = load_data(config.path.train_data)
    df_valid = load_data(config.path.valid_data)

    train_dataset_raw = process_dataset_for_training(df_train)
    valid_dataset_raw = process_dataset_for_training(df_valid)

    print(f"  - Train samples: {len(train_dataset_raw)}")
    print(f"  - Valid samples: {len(valid_dataset_raw)}")

    # print(f"ğŸ’½ Train Data Format: {train_dataset_raw['messages'][0:2]}")
    # print(f"ğŸ’½ Valid Data Format: {valid_dataset_raw['messages'][0:2]}")

    # 3. ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
    print(f"\nğŸ¤– ëª¨ë¸ ë¡œë“œ ì¤‘: {config.model.model_name}")

    model, tokenizer = FastLanguageModel.from_pretrained(
        config.model.model_name,
        dtype=get_torch_dtype(config.model.torch_dtype),
        # trust_remote_code=config.model.trust_remote_code,
        load_in_4bit=True,
        max_seq_length=config.training.max_seq_length,   # âœ… ì¶”ê°€
    )

    tokenizer = setup_tokenizer(tokenizer)

    print("  - ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ")

    # 4. í† í°í™”
    print(f"\nğŸ“ í† í°í™” ì¤‘ (max_seq_length: {config.training.max_seq_length})...")
    # tokenized_dataset = tokenize_dataset(
    #     processed_dataset,
    #     tokenizer,
    #     max_seq_length=config.training.max_seq_length
    # )

    tokenized_train = tokenize_dataset(
        train_dataset_raw,
        tokenizer,
        max_seq_length=config.training.max_seq_length
    )

    tokenized_valid = tokenize_dataset(
        valid_dataset_raw,
        tokenizer,
        max_seq_length=config.training.max_seq_length
    )


    """

    # Train/Eval ë¶„í• 
    split_dataset = tokenized_dataset.train_test_split(
        test_size=config.training.test_size,
        seed=config.training.seed
    )
    train_dataset = split_dataset['train']
    eval_dataset = split_dataset['test']

    print(f"  - Train: {len(train_dataset)} samples")
    print(f"  - Eval: {len(eval_dataset)} samples")

    """

    # í† í° í†µê³„
    # stats = get_token_statistics(tokenized_dataset, tokenizer)
    # print(f"  - Token ê¸¸ì´: min={stats['min']}, max={stats['max']}, mean={stats['mean']:.1f}")

    stats = get_token_statistics(tokenized_train, tokenizer)
    print(f"  - Token ê¸¸ì´(train): min={stats['min']}, max={stats['max']}, mean={stats['mean']:.1f}")

    # print(tokenized_dataset.column_names)
    # print(tokenized_dataset[0].keys())

    # 5. LoRA ì„¤ì •
    print(f"\nâš™ï¸ LoRA ì„¤ì •")
    model = FastLanguageModel.get_peft_model(
        model,
        r=config.lora.r,
        target_modules=config.lora.target_modules,
        lora_alpha=config.lora.lora_alpha,
        lora_dropout=config.lora.lora_dropout,
        bias=config.lora.bias,# âœ… ì—¬ê¸° 2ê°œ ì¶”ê°€
        use_gradient_checkpointing=config.lora.use_gradient_checkpointing,  # "unsloth"
        use_rslora=config.lora.use_rslora,

        random_state=config.training.seed,
        max_seq_length=config.training.max_seq_length,
        loftq_config=None
    )
    print(f"  - r: {config.lora.r}, alpha: {config.lora.lora_alpha}")
    print(f"  - target_modules: {config.lora.target_modules}")


    # 6. ë©”íŠ¸ë¦­ í•¨ìˆ˜ ìƒì„±
    preprocess_logits_for_metrics, compute_metrics = create_metric_functions(tokenizer)

    # 7. SFTConfig ì„¤ì •
    print(f"\nğŸ“‹ í•™ìŠµ ì„¤ì •")
    print(f"  - epochs: {config.training.num_train_epochs}")
    print(f"  - batch_size: {config.training.per_device_train_batch_size}")
    print(f"  - learning_rate: {config.training.learning_rate}")
    print(f"  - output_dir: {config.path.output_dir}")

    sft_config = SFTConfig(
        # do_train=True,
        # do_eval=True,
        per_device_train_batch_size=config.training.per_device_train_batch_size,
        gradient_accumulation_steps=config.training.gradient_accumulation_steps,
        warmup_steps=config.training.warmup_steps,
        warmup_ratio=config.training.warmup_ratio,
        max_steps=config.training.max_steps,
        num_train_epochs=config.training.num_train_epochs,
        learning_rate=config.training.learning_rate,
        fp16=not is_bfloat16_supported(),
        bf16=is_bfloat16_supported(),
        logging_steps=config.training.logging_steps,
        optim=config.training.optim,
        weight_decay=config.training.weight_decay,
        lr_scheduler_type=config.training.lr_scheduler_type,
        seed = config.training.seed,
        output_dir=config.path.output_dir,

        per_device_eval_batch_size=config.training.per_device_eval_batch_size,

        # # âœ… ì €ì¥/í‰ê°€/ë² ìŠ¤íŠ¸
        # save_strategy="epoch",
        # eval_strategy="epoch",                 # transformers ìµœì‹ ì€ eval_strategy
        # save_total_limit=2,                    # best ë‚ ì•„ê°€ëŠ” ê²ƒ ë°©ì§€
        # metric_for_best_model="eval_f1",
        # greater_is_better=True,
        # load_best_model_at_end=True,

        # âœ… config ê°’ ê·¸ëŒ€ë¡œ ì‚¬ìš©
        save_strategy=config.training.save_strategy,
        eval_strategy=config.training.eval_strategy,   # ë„ˆ ì½”ë“œê°€ eval_strategy ì“°ê³  ìˆìœ¼ë‹ˆ ê·¸ëŒ€ë¡œ
        eval_steps = config.training.eval_steps,          # ì¶”ì²œ: 100~300 ì‚¬ì´
        save_steps = config.training.save_steps,          # eval_stepsë‘ ë™ì¼í•˜ê²Œ

        save_total_limit=config.training.save_total_limit, # best + last ë‚¨ê¸°ë ¤ë©´ 2 ì´ìƒ


        # âœ… best ëª¨ë¸ ì €ì¥ (ë„¤ê°€ ì¶”ê°€í•œ 3ê°œê°€ ì—¬ê¸°ë¡œ ë“¤ì–´ì™€ì•¼ ì ìš©ë¨)
        load_best_model_at_end=config.training.load_best_model_at_end,
        metric_for_best_model=config.training.metric_for_best_model,
        greater_is_better=config.training.greater_is_better,
        save_only_model=config.training.save_only_model,

    )

    data_collator = CompletionOnlyDataCollator(
        tokenizer,
        response_template="<|im_start|>assistant"
    )
    # 8. Trainer ìƒì„±
    trainer = SFTTrainer(
        model=model,
        tokenizer = tokenizer,
        train_dataset=tokenized_train,
        eval_dataset=tokenized_valid,
        data_collator=data_collator,
        compute_metrics=compute_metrics,
        preprocess_logits_for_metrics=preprocess_logits_for_metrics,
        args=sft_config,
        callbacks=[AddStepToLogsCallback()],
    )
    print("collator:", type(trainer.data_collator))
    print_trainer_args(trainer)

    # DataCollator ê²€ì¦ìš© ìƒ˜í”Œ ë°°ì¹˜ ì¶œë ¥
    dl = DataLoader(
        tokenized_train.select(range(1)),
        batch_size=1,
        collate_fn=trainer.data_collator,
    )
    batch = next(iter(dl))

    input_ids = batch["input_ids"][0].tolist()
    labels = batch["labels"][0].tolist()

    full_text = tokenizer.decode(input_ids, skip_special_tokens=False)

    loss_pos = [i for i, t in enumerate(labels) if t != -100]
    loss_tokens = [input_ids[i] for i in loss_pos]
    loss_text = tokenizer.decode(loss_tokens, skip_special_tokens=False)

    print("\n" + "="*80)
    print("ğŸ” FULL INPUT:")
    print(full_text)
    print("-"*80)
    print("ğŸ” LOSS TOKENS ONLY:")
    print(loss_text)
    print("="*80 + "\n")


    # 9. í•™ìŠµ ì‹¤í–‰
    print("\n" + "=" * 60)
    print("ğŸƒ í•™ìŠµ ì‹¤í–‰ ì¤‘...")
    print("=" * 60)

    train_result = trainer.train()

    trainer.save_model(os.path.join(config.path.output_dir, "checkpoint-last"))# (ì„ íƒ) ìµœì¢… ëª¨ë¸ ì €ì¥


    # 10. ê²°ê³¼ ì¶œë ¥
    print("\n" + "=" * 60)
    print("âœ… í•™ìŠµ ì™„ë£Œ!")
    print("=" * 60)
    print(f"  - Total steps: {train_result.global_step}")
    print(f"  - Train loss: {train_result.training_loss:.4f}")
    print(f"  - ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ìœ„ì¹˜: {config.path.output_dir}")

    return trainer


# =============================================================================
# ë©”ì¸ ì‹¤í–‰
# =============================================================================

def main():
    parser = argparse.ArgumentParser(description="Train the model")
    parser.add_argument(
        "--exp",
        type=str,
        default=None,
        help="ì‹¤í—˜ ì´ë¦„ (large_context, more_lora, longer_training)"
    )
    parser.add_argument(
        "--output_dir",
        type=str,
        default=None,
        help="ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ê²½ë¡œ"
    )
    parser.add_argument(
        "--epochs",
        type=int,
        default=None,
        help="í•™ìŠµ ì—í­ ìˆ˜"
    )
    parser.add_argument(
        "--lr",
        type=float,
        default=None,
        help="í•™ìŠµë¥ "
    )
    parser.add_argument(
        "--max_seq_length",
        type=int,
        default=None,
        help="ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´"
    )

    args = parser.parse_args()

    # ì„¤ì • ë¡œë“œ
    if args.exp:
        config = get_experiment_config(args.exp)
        print(f"ğŸ“Œ ì‹¤í—˜ ì„¤ì • ë¡œë“œ: {args.exp}")
    else:
        config = get_config()
        print("ğŸ“Œ ê¸°ë³¸ ì„¤ì • ì‚¬ìš©")

    # ëª…ë ¹ì¤„ ì¸ìë¡œ ì˜¤ë²„ë¼ì´ë“œ
    if args.output_dir:
        config.path.output_dir = args.output_dir
    if args.epochs:
        config.training.num_train_epochs = args.epochs
    if args.lr:
        config.training.learning_rate = args.lr
    if args.max_seq_length:
        config.training.max_seq_length = args.max_seq_length

    # í•™ìŠµ ì‹¤í–‰
    train(config)


if __name__ == "__main__":
    main()
