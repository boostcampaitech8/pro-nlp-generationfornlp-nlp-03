# ğŸ¯ Generation for NLP Baseline

í•œêµ­ì–´ ê°ê´€ì‹ ë¬¸ì œ í’€ì´ ë² ì´ìŠ¤ë¼ì¸ ì½”ë“œì…ë‹ˆë‹¤.

## ğŸ“ íŒŒì¼ êµ¬ì¡°

```
baseline/
â”œâ”€â”€ config.py        # ì„¤ì • íŒŒì¼ (í•˜ì´í¼íŒŒë¼ë¯¸í„°, ê²½ë¡œ)
â”œâ”€â”€ data_utils.py    # ë°ì´í„° ì²˜ë¦¬ ìœ í‹¸ë¦¬í‹°
â”œâ”€â”€ train.py         # í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ inference.py     # ì¶”ë¡  ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ requirements.txt # ì˜ì¡´ì„± íŒ¨í‚¤ì§€
â””â”€â”€ README.md        # ì‚¬ìš© ê°€ì´ë“œ
```

## ğŸš€ Quick Start

### 1. í™˜ê²½ ì„¤ì •

```bash
# ê°€ìƒí™˜ê²½ ìƒì„± (ê¶Œì¥)
python3.10 -m venv venv
source venv/bin/activate

# íŒ¨í‚¤ì§€ ì„¤ì¹˜
pip install -r requirements.txt
```

### 2. ë°ì´í„° ì¤€ë¹„

`train.csv`ì™€ `test.csv`ë¥¼ í˜„ì¬ ë””ë ‰í† ë¦¬ì— ìœ„ì¹˜ì‹œí‚¤ì„¸ìš”.

### 3. í•™ìŠµ

```bash
# ê¸°ë³¸ í•™ìŠµ
python train.py

# ì‹¤í—˜ ì„¤ì •ìœ¼ë¡œ í•™ìŠµ
python train.py --exp large_context

# ì»¤ìŠ¤í…€ ì„¤ì •
python train.py --epochs 5 --lr 1e-5 --max_seq_length 2048
```

### 4. ì¶”ë¡ 

```bash
# ê¸°ë³¸ ì¶”ë¡ 
python inference.py --checkpoint outputs/checkpoint-4491

# ì»¤ìŠ¤í…€ ì¶œë ¥ ê²½ë¡œ
python inference.py --checkpoint outputs/checkpoint-4491 --output my_result.csv
```

---

## âš™ï¸ ì„¤ì • ë³€ê²½í•˜ê¸°

### ë°©ë²• 1: config.py ì§ì ‘ ìˆ˜ì •

```python
# config.py ì—ì„œ ì§ì ‘ ìˆ˜ì •
@dataclass
class TrainingConfig:
    num_train_epochs: int = 5      # 3 â†’ 5
    learning_rate: float = 1e-5    # 2e-5 â†’ 1e-5
```

### ë°©ë²• 2: ëª…ë ¹ì¤„ ì¸ì ì‚¬ìš©

```bash
python train.py --epochs 5 --lr 1e-5 --max_seq_length 2048 --output_dir my_exp
```

### ë°©ë²• 3: ì‹¤í—˜ í”„ë¦¬ì…‹ ì‚¬ìš©

```bash
# ì‚¬ì „ ì •ì˜ëœ ì‹¤í—˜ ì„¤ì • ì‚¬ìš©
python train.py --exp large_context   # ë” ê¸´ ì»¨í…ìŠ¤íŠ¸
python train.py --exp more_lora       # ë” ë§ì€ LoRA ëª¨ë“ˆ
python train.py --exp longer_training # ë” ê¸´ í•™ìŠµ
```

---

## ğŸ”¬ ì‹¤í—˜ ê°€ì´ë“œ

### ì‹¤í—˜ 1: ë” ê¸´ ì‹œí€€ìŠ¤ ì²˜ë¦¬

```python
# config.py ìˆ˜ì •
config.training.max_seq_length = 2048  # 1024 â†’ 2048
config.training.per_device_train_batch_size = 1
config.training.gradient_accumulation_steps = 2  # ë©”ëª¨ë¦¬ ì ˆì•½
```

ë˜ëŠ”:
```bash
python train.py --exp large_context
```

### ì‹¤í—˜ 2: LoRA í™•ì¥

```python
# config.py ìˆ˜ì •
config.lora.r = 16  # 6 â†’ 16
config.lora.lora_alpha = 32  # 8 â†’ 32
config.lora.target_modules = ['q_proj', 'k_proj', 'v_proj', 'o_proj']
```

ë˜ëŠ”:
```bash
python train.py --exp more_lora
```

### ì‹¤í—˜ 3: ë‹¤ë¥¸ ëª¨ë¸ ì‚¬ìš©

```python
# config.py ìˆ˜ì •
config.model.model_name = "beomi/gemma-ko-7b"  # ë” í° ëª¨ë¸
```

### ì‹¤í—˜ 4: Learning Rate ìŠ¤ì¼€ì¤„ ë³€ê²½

```python
# config.py ìˆ˜ì •
config.training.lr_scheduler_type = "linear"  # "cosine" â†’ "linear"
config.training.warmup_ratio = 0.1  # warmup ì¶”ê°€
```

---

## ğŸ“Š ì„±ëŠ¥ ê°œì„  íŒíŠ¸

1. **ë°ì´í„°**
   - `max_seq_length` ëŠ˜ë¦¬ê¸° (1024 ì´ˆê³¼ ë°ì´í„° í¬í•¨)
   - ë°ì´í„° ì¦ê°• (ì„ íƒì§€ ìˆœì„œ ì„ê¸°)

2. **ëª¨ë¸**
   - ë” í° ëª¨ë¸ ì‚¬ìš© (gemma-ko-7b)
   - LoRA rank(r) ì¦ê°€

3. **í•™ìŠµ**
   - ì—í­ ìˆ˜ ì¦ê°€
   - Learning rate ì¡°ì •
   - Gradient accumulation í™œìš©

4. **ì•™ìƒë¸”**
   - ì—¬ëŸ¬ ì²´í¬í¬ì¸íŠ¸ ê²°ê³¼ ì¡°í•©

---

## ğŸ› Troubleshooting

### CUDA Out of Memory

```python
# ë°°ì¹˜ ì‚¬ì´ì¦ˆ ì¤„ì´ê¸°
config.training.per_device_train_batch_size = 1

# Gradient accumulation ì‚¬ìš©
config.training.gradient_accumulation_steps = 4

# ì‹œí€€ìŠ¤ ê¸¸ì´ ì¤„ì´ê¸°
config.training.max_seq_length = 512
```

### í•™ìŠµì´ ëŠë¦´ ë•Œ

```python
# Mixed precision ì‚¬ìš© (ì´ë¯¸ float16)
config.model.torch_dtype = "bfloat16"  # A100 ë“±ì—ì„œ

# ë¡œê¹… ë¹ˆë„ ì¤„ì´ê¸°
config.training.logging_steps = 50
```

---

## ğŸ“ ì£¼ìš” íŒŒë¼ë¯¸í„° ì„¤ëª…

| íŒŒë¼ë¯¸í„° | ê¸°ë³¸ê°’ | ì„¤ëª… |
|---------|--------|------|
| `max_seq_length` | 1024 | ìµœëŒ€ ì…ë ¥ í† í° ìˆ˜ |
| `num_train_epochs` | 3 | í•™ìŠµ ì—í­ ìˆ˜ |
| `learning_rate` | 2e-5 | í•™ìŠµë¥  |
| `lora.r` | 6 | LoRA rank (í‘œí˜„ë ¥) |
| `lora.lora_alpha` | 8 | LoRA ìŠ¤ì¼€ì¼ë§ |
| `lora.target_modules` | q_proj, k_proj | í•™ìŠµ ëŒ€ìƒ ëª¨ë“ˆ |

---

## ğŸ“Œ ë² ì´ìŠ¤ë¼ì¸ ì„±ëŠ¥

- Validation Accuracy: ~47%
- í•™ìŠµ ì‹œê°„: ~18ë¶„ (3 epochs)
- ì¶”ë¡  ì‹œê°„: ~15ë¶„

---

## License

This code is for educational purposes.
