"""
í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
Usage:
    python train.py
    python train.py --exp large_context
"""
import sys
import unsloth
import os
import argparse
import torch
import numpy as np
import random
import evaluate

from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments
from trl import SFTTrainer, SFTConfig
from peft import LoraConfig
from unsloth import FastLanguageModel, is_bfloat16_supported, UnslothTrainer, UnslothTrainingArguments

# from code.config import get_config, get_experiment_config
from config import get_config, get_experiment_config
from data_utils import (
    load_data,
    process_dataset_for_training,
    setup_tokenizer,
    tokenize_dataset,
    get_token_statistics
)


# =============================================================================
# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# =============================================================================

def set_seed(seed: int):
    """ì¬í˜„ì„±ì„ ìœ„í•œ ì‹œë“œ ê³ ì •"""
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    np.random.seed(seed)
    random.seed(seed)
    print(f"âœ“ Seed ê³ ì •: {seed}")


def get_torch_dtype(dtype_str: str):
    """ë¬¸ìì—´ì„ torch dtypeìœ¼ë¡œ ë³€í™˜"""
    dtype_map = {
        "float16": torch.float16,
        "bfloat16": torch.bfloat16,
        "float32": torch.float32,
    }
    return dtype_map.get(dtype_str, torch.float16)


# =============================================================================
# ë©”íŠ¸ë¦­ í•¨ìˆ˜
# =============================================================================
def create_metric_functions(tokenizer):
    import evaluate
    f1_metric = evaluate.load("f1")
    int_output_map = {"1": 0, "2": 1, "3": 2, "4": 3, "5": 4}

    # âœ… '1'~'5' í† í° idë¥¼ ì•ˆì „í•˜ê²Œ êµ¬í•˜ê¸° (tokenizer.vocab ì˜ì¡´ X)
    choice_token_ids = []
    for s in ["1", "2", "3", "4", "5"]:
        tid = tokenizer.encode(s, add_special_tokens=False)
        if len(tid) != 1:
            # ìˆ«ìê°€ í•œ í† í°ì´ ì•„ë‹ ìˆ˜ë„ ìˆìŒ. ì´ ê²½ìš°ëŠ” ë³„ë„ ì²˜ë¦¬ í•„ìš”.
            raise ValueError(f"í† í°í™”ê°€ ì˜ˆìƒê³¼ ë‹¬ë¼ìš”: '{s}' -> {tid}. ìˆ«ìë§Œ 1í† í°ì´ ì•„ë‹ˆë©´ ë¡œì§ì„ ë°”ê¿”ì•¼ í•´ìš”.")
        choice_token_ids.append(tid[0])

    def preprocess_logits_for_metrics(logits, labels):
        """
        âœ… labelsì—ì„œ ì •ë‹µ í† í° ìœ„ì¹˜ë¥¼ ì°¾ì•„ì„œ ê·¸ ìœ„ì¹˜ì˜ logitsë§Œ ë½‘ì•„ì˜¨ë‹¤.
        ë°˜í™˜ shape: (batch, 5)  -> (1~5ì— ëŒ€í•œ ì ìˆ˜)
        """
        logits = logits if not isinstance(logits, tuple) else logits[0]  # (B, T, V)
        B, T, V = logits.shape

        # labels: (B, T)
        labels_t = labels

        # ì •ë‹µ ìœ„ì¹˜ idxë¥¼ ë°°ì¹˜ë§ˆë‹¤ êµ¬í•˜ê¸°: labels != -100 ì¸ ìœ„ì¹˜ ì¤‘ "ì²« ë²ˆì§¸"
        # (ì§€ê¸ˆì€ ë‹µ í† í° + <|im_end|> 2ê°œê°€ ë‚¨ì•„ìˆìœ¼ë‹ˆ ì²« ë²ˆì§¸ê°€ ë‹µ)
        ans_pos = []
        for i in range(B):
            positions = (labels_t[i] != -100).nonzero(as_tuple=False).squeeze(-1)
            if positions.numel() == 0:
                # í˜¹ì‹œ ì „ë¶€ -100ì´ë©´ ì•ˆì „í•˜ê²Œ ë§ˆì§€ë§‰ í† í°ìœ¼ë¡œ(ê·¼ë° ì´ëŸ° ìƒ˜í”Œì€ metricì—ì„œ ì‚¬ì‹¤ìƒ ë¬´ì˜ë¯¸)
                ans_pos.append(T - 1)
            else:
                ans_pos.append(int(positions[0].item()))
        ans_pos = torch.tensor(ans_pos, device=logits.device)  # (B,)

        # ë°°ì¹˜ ì¸ë±ì‹±ìœ¼ë¡œ ê° ìƒ˜í”Œì˜ ì •ë‹µ ìœ„ì¹˜ logitsì„ ë½‘ëŠ”ë‹¤: (B, V)
        batch_idx = torch.arange(B, device=logits.device)
        ans_logits = logits[batch_idx, ans_pos, :]  # (B, V)

        # ê·¸ ì¤‘ 1~5 í† í° idë§Œ ì¶”ì¶œ: (B, 5)
        ans_logits_5 = ans_logits[:, choice_token_ids]
        return ans_logits_5

    def compute_metrics(eval_pred):
        """
        preprocess_logits_for_metricsê°€ ì´ë¯¸ (B,5) logitsì„ ë„˜ê²¨ì¤Œ.
        labelsì—ì„œ ì •ë‹µ ìˆ«ìë„ ë˜‘ê°™ì´ labels ê¸°ë°˜ìœ¼ë¡œ ì¶”ì¶œí•´ì„œ ë¹„êµ.
        """
        logits_5, labels = eval_pred  # logits_5: (B,5)

        # labelsì—ì„œ ì •ë‹µ í† í° id ì¶”ì¶œ (labels != -100ì˜ ì²« ë²ˆì§¸ í† í°)
        B, T = labels.shape
        y_true = []
        for i in range(B):
            positions = np.where(labels[i] != -100)[0]
            if len(positions) == 0:
                y_true.append(0)
            else:
                tid = int(labels[i, positions[0]])
                # tid -> "1~5"ë¡œ ë§¤í•‘
                if tid in choice_token_ids:
                    y_true.append(choice_token_ids.index(tid))
                else:
                    # ì˜ˆìƒ ë°–ì´ë©´ 0 ì²˜ë¦¬
                    y_true.append(0)

        probs = torch.softmax(torch.tensor(logits_5), dim=-1)
        y_pred = torch.argmax(probs, dim=-1).cpu().numpy()

        return f1_metric.compute(predictions=y_pred, references=y_true, average="macro")

    return preprocess_logits_for_metrics, compute_metrics

# def create_metric_functions(tokenizer):
#     """ë©”íŠ¸ë¦­ ê³„ì‚° í•¨ìˆ˜ë“¤ ìƒì„±"""

#     f1_metric = evaluate.load("f1")
#     int_output_map = {"1": 0, "2": 1, "3": 2, "4": 3, "5": 4}

#     def preprocess_logits_for_metrics(logits, labels):
#         """ì •ë‹µ í† í° ìœ„ì¹˜ì˜ logitsë§Œ ì¶”ì¶œ"""
#         logits = logits if not isinstance(logits, tuple) else logits[0]
#         logit_idx = [
#             tokenizer.vocab["1"],
#             tokenizer.vocab["2"],
#             tokenizer.vocab["3"],
#             tokenizer.vocab["4"],
#             tokenizer.vocab["5"]
#         ]
#         logits = logits[:, -2, logit_idx]  # -2: answer token, -1: eos token
#         return logits

#     def compute_metrics(evaluation_result):
#         """ì •í™•ë„ ê³„ì‚°"""
#         logits, labels = evaluation_result

#         # í† í°í™”ëœ ë ˆì´ë¸” ë””ì½”ë”©
#         labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
#         labels = tokenizer.batch_decode(labels, skip_special_tokens=True)
#         labels = list(map(lambda x: x.split("<end_of_turn>")[0].strip(), labels))
#         labels = list(map(lambda x: int_output_map.get(x, 0), labels))

#         # Softmaxë¡œ í™•ë¥  ë³€í™˜
#         probs = torch.nn.functional.softmax(torch.tensor(logits), dim=-1)
#         predictions = np.argmax(probs, axis=-1)

#         # ì •í™•ë„ ê³„ì‚°
#         macro_f1 = f1_metric.compute(predictions=predictions, references=labels, average="macro")
#         return macro_f1

#     return preprocess_logits_for_metrics, compute_metrics


# =============================================================================
# ë©”ì¸ í•™ìŠµ í•¨ìˆ˜
# =============================================================================

def train(config):
    """ëª¨ë¸ í•™ìŠµ ì‹¤í–‰"""

    print("=" * 60)
    print("ğŸš€ í•™ìŠµ ì‹œì‘")
    print("=" * 60)

    # 1. ì‹œë“œ ê³ ì •
    set_seed(config.training.seed)

    # 2. ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
    print("\nğŸ“‚ ë°ì´í„° ë¡œë“œ ì¤‘...")
    # df = load_data(config.path.train_data)
    # âœ… train / valid ë°ì´í„° ë‘˜ ë‹¤ ë¡œë“œ
    df_train = load_data(config.path.train_data)
    df_valid = load_data(config.path.valid_data)

    train_dataset_raw = process_dataset_for_training(df_train)
    valid_dataset_raw = process_dataset_for_training(df_valid)

    print(f"  - Train samples: {len(train_dataset_raw)}")
    print(f"  - Valid samples: {len(valid_dataset_raw)}")

    # print(f"  - ì´ ë°ì´í„° ìˆ˜: {len(df)}")

    # processed_dataset = process_dataset_for_training(df)
    # print(f"  - ì „ì²˜ë¦¬ ì™„ë£Œ: {len(processed_dataset)} samples")
    # print(f"ğŸ’½ Data Format{processed_dataset['messages'][0:4]}")

    # print(f"ğŸ’½ Train Data Format: {train_dataset_raw['messages'][0:2]}")
    # print(f"ğŸ’½ Valid Data Format: {valid_dataset_raw['messages'][0:2]}")

    # 3. ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
    print(f"\nğŸ¤– ëª¨ë¸ ë¡œë“œ ì¤‘: {config.model.model_name}")

    model, tokenizer = FastLanguageModel.from_pretrained(
        config.model.model_name,
        dtype=get_torch_dtype(config.model.torch_dtype),
        # trust_remote_code=config.model.trust_remote_code,
        load_in_4bit=True,
        max_seq_length=config.training.max_seq_length,   # âœ… ì¶”ê°€
    )

    tokenizer = setup_tokenizer(tokenizer)

    print("  - ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ")

    # 4. í† í°í™”
    print(f"\nğŸ“ í† í°í™” ì¤‘ (max_seq_length: {config.training.max_seq_length})...")
    # tokenized_dataset = tokenize_dataset(
    #     processed_dataset,
    #     tokenizer,
    #     max_seq_length=config.training.max_seq_length
    # )

    tokenized_train = tokenize_dataset(
    train_dataset_raw,
    tokenizer,
    max_seq_length=config.training.max_seq_length
    )

    tokenized_valid = tokenize_dataset(
        valid_dataset_raw,
        tokenizer,
        max_seq_length=config.training.max_seq_length
    )


    """

    # Train/Eval ë¶„í• 
    split_dataset = tokenized_dataset.train_test_split(
        test_size=config.training.test_size,
        seed=config.training.seed
    )
    train_dataset = split_dataset['train']
    eval_dataset = split_dataset['test']

    print(f"  - Train: {len(train_dataset)} samples")
    print(f"  - Eval: {len(eval_dataset)} samples")

    """

    # í† í° í†µê³„
    # stats = get_token_statistics(tokenized_dataset, tokenizer)
    # print(f"  - Token ê¸¸ì´: min={stats['min']}, max={stats['max']}, mean={stats['mean']:.1f}")

    stats = get_token_statistics(tokenized_train, tokenizer)
    print(f"  - Token ê¸¸ì´(train): min={stats['min']}, max={stats['max']}, mean={stats['mean']:.1f}")

    # print(tokenized_dataset.column_names)
    # print(tokenized_dataset[0].keys())

    # 5. LoRA ì„¤ì •
    print(f"\nâš™ï¸ LoRA ì„¤ì •")
    model = FastLanguageModel.get_peft_model(
        model,
        r=config.lora.r,
        target_modules=config.lora.target_modules,
        lora_alpha=config.lora.lora_alpha,
        lora_dropout=config.lora.lora_dropout,
        bias=config.lora.bias,# âœ… ì—¬ê¸° 2ê°œ ì¶”ê°€
        use_gradient_checkpointing=config.lora.use_gradient_checkpointing,  # "unsloth"
        use_rslora=config.lora.use_rslora,
        use_dora=config.lora.use_dora,

        init_lora_weights="pissa",

        random_state=config.training.seed,
        max_seq_length=config.training.max_seq_length,
        loftq_config=None
    )
    print(f"  - r: {config.lora.r}, alpha: {config.lora.lora_alpha}")
    print(f"  - target_modules: {config.lora.target_modules}")


    # 6. ë©”íŠ¸ë¦­ í•¨ìˆ˜ ìƒì„±
    preprocess_logits_for_metrics, compute_metrics = create_metric_functions(tokenizer)

    # 7. SFTConfig ì„¤ì •
    print(f"\nğŸ“‹ í•™ìŠµ ì„¤ì •")
    print(f"  - epochs: {config.training.num_train_epochs}")
    print(f"  - batch_size: {config.training.per_device_train_batch_size}")
    print(f"  - learning_rate: {config.training.learning_rate}")
    print(f"  - output_dir: {config.path.output_dir}")

    sft_config = UnslothTrainingArguments(
        # do_train=True,
        # do_eval=True,
        per_device_train_batch_size=config.training.per_device_train_batch_size,
        gradient_accumulation_steps=config.training.gradient_accumulation_steps,
        warmup_steps=config.training.warmup_steps,
        warmup_ratio=config.training.warmup_ratio,
        max_steps=config.training.max_steps,
        num_train_epochs=config.training.num_train_epochs,
        learning_rate=config.training.learning_rate,
        fp16=not is_bfloat16_supported(),
        bf16=is_bfloat16_supported(),
        logging_steps=config.training.logging_steps,
        optim=config.training.optim,
        weight_decay=config.training.weight_decay,
        lr_scheduler_type=config.training.lr_scheduler_type,
        seed = config.training.seed,
        output_dir=config.path.output_dir,

        per_device_eval_batch_size=config.training.per_device_eval_batch_size,

        # # âœ… ì €ì¥/í‰ê°€/ë² ìŠ¤íŠ¸
        # save_strategy="epoch",
        # eval_strategy="epoch",                 # transformers ìµœì‹ ì€ eval_strategy
        # save_total_limit=2,                    # best ë‚ ì•„ê°€ëŠ” ê²ƒ ë°©ì§€
        # metric_for_best_model="eval_f1",
        # greater_is_better=True,
        # load_best_model_at_end=True,
        # âœ… config ê°’ ê·¸ëŒ€ë¡œ ì‚¬ìš©
        save_strategy=config.training.save_strategy,
        eval_strategy=config.training.eval_strategy,   # ë„ˆ ì½”ë“œê°€ eval_strategy ì“°ê³  ìˆìœ¼ë‹ˆ ê·¸ëŒ€ë¡œ
        save_total_limit=config.training.save_total_limit,

        # âœ… best ëª¨ë¸ ì €ì¥ (ë„¤ê°€ ì¶”ê°€í•œ 3ê°œê°€ ì—¬ê¸°ë¡œ ë“¤ì–´ì™€ì•¼ ì ìš©ë¨)
        load_best_model_at_end=config.training.load_best_model_at_end,
        metric_for_best_model=config.training.metric_for_best_model,
        greater_is_better=config.training.greater_is_better,
    )

    # 8. Trainer ìƒì„±
    trainer = UnslothTrainer(
        model=model,
        tokenizer = tokenizer,
        train_dataset=tokenized_train,
        eval_dataset=tokenized_valid,
        compute_metrics=compute_metrics,
        preprocess_logits_for_metrics=preprocess_logits_for_metrics,
        args=sft_config,
    )

    # 9. í•™ìŠµ ì‹¤í–‰
    print("\n" + "=" * 60)
    print("ğŸƒ í•™ìŠµ ì‹¤í–‰ ì¤‘...")
    print("=" * 60)

    train_result = trainer.train()

    print("best_ckpt:", trainer.state.best_model_checkpoint)
    print("best_metric:", trainer.state.best_metric)

    trainer.save_model(config.path.output_dir)  # (ì„ íƒ) ìµœì¢… ëª¨ë¸ ì €ì¥


    # 10. ê²°ê³¼ ì¶œë ¥
    print("\n" + "=" * 60)
    print("âœ… í•™ìŠµ ì™„ë£Œ!")
    print("=" * 60)
    print(f"  - Total steps: {train_result.global_step}")
    print(f"  - Train loss: {train_result.training_loss:.4f}")
    print(f"  - ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ìœ„ì¹˜: {config.path.output_dir}")

    return trainer


# =============================================================================
# ë©”ì¸ ì‹¤í–‰
# =============================================================================

def main():
    parser = argparse.ArgumentParser(description="Train the model")
    parser.add_argument(
        "--exp",
        type=str,
        default=None,
        help="ì‹¤í—˜ ì´ë¦„ (large_context, more_lora, longer_training)"
    )
    parser.add_argument(
        "--output_dir",
        type=str,
        default=None,
        help="ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ê²½ë¡œ"
    )
    parser.add_argument(
        "--epochs",
        type=int,
        default=None,
        help="í•™ìŠµ ì—í­ ìˆ˜"
    )
    parser.add_argument(
        "--lr",
        type=float,
        default=None,
        help="í•™ìŠµë¥ "
    )
    parser.add_argument(
        "--max_seq_length",
        type=int,
        default=None,
        help="ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´"
    )

    args = parser.parse_args()

    # ì„¤ì • ë¡œë“œ
    if args.exp:
        config = get_experiment_config(args.exp)
        print(f"ğŸ“Œ ì‹¤í—˜ ì„¤ì • ë¡œë“œ: {args.exp}")
    else:
        config = get_config()
        print("ğŸ“Œ ê¸°ë³¸ ì„¤ì • ì‚¬ìš©")

    # ëª…ë ¹ì¤„ ì¸ìë¡œ ì˜¤ë²„ë¼ì´ë“œ
    if args.output_dir:
        config.path.output_dir = args.output_dir
    if args.epochs:
        config.training.num_train_epochs = args.epochs
    if args.lr:
        config.training.learning_rate = args.lr
    if args.max_seq_length:
        config.training.max_seq_length = args.max_seq_length

    # í•™ìŠµ ì‹¤í–‰
    train(config)


if __name__ == "__main__":
    main()