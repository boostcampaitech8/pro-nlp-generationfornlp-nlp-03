"""
í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
Usage:
    python train.py
    python train.py --exp large_context
"""
import sys
import unsloth
import re
from typing import Optional
import os
import argparse
import torch
import numpy as np
import random
import evaluate

from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments
from trl import SFTTrainer, SFTConfig
from peft import LoraConfig
from unsloth import FastLanguageModel, is_bfloat16_supported, UnslothTrainer, UnslothTrainingArguments

# from code.config import get_config, get_experiment_config
from config import get_config, get_experiment_config
from data_utils import (
    load_data,
    process_dataset_for_training,
    setup_tokenizer,
    tokenize_dataset,
    get_token_statistics
)


# =============================================================================
# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# =============================================================================

def set_seed(seed: int):
    """ì¬í˜„ì„±ì„ ìœ„í•œ ì‹œë“œ ê³ ì •"""
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    np.random.seed(seed)
    random.seed(seed)
    print(f"âœ“ Seed ê³ ì •: {seed}")


def get_torch_dtype(dtype_str: str):
    """ë¬¸ìì—´ì„ torch dtypeìœ¼ë¡œ ë³€í™˜"""
    dtype_map = {
        "float16": torch.float16,
        "bfloat16": torch.bfloat16,
        "float32": torch.float32,
    }
    return dtype_map.get(dtype_str, torch.float16)

def _get_latest_checkpoint(output_dir: str) -> Optional[str]:
    """
    output_dir ì•ˆì˜ checkpoint-* ì¤‘ ê°€ì¥ í° stepì„ ë°˜í™˜
    ì˜ˆ) checkpoint-256, checkpoint-512 ...
    """
    if not output_dir or not os.path.isdir(output_dir):
        return None

    candidates = []
    for name in os.listdir(output_dir):
        if name.startswith("checkpoint-"):
            m = re.match(r"checkpoint-(\d+)", name)
            if m:
                candidates.append((int(m.group(1)), os.path.join(output_dir, name)))

    if not candidates:
        return None
    candidates.sort(key=lambda x: x[0])
    return candidates[-1][1]

# =============================================================================
# ë©”íŠ¸ë¦­ í•¨ìˆ˜
# =============================================================================
def create_metric_functions(tokenizer):
    import evaluate
    f1_metric = evaluate.load("f1")
    int_output_map = {"1": 0, "2": 1, "3": 2, "4": 3, "5": 4}

    # âœ… '1'~'5' í† í° idë¥¼ ì•ˆì „í•˜ê²Œ êµ¬í•˜ê¸° (tokenizer.vocab ì˜ì¡´ X)
    choice_token_ids = []
    for s in ["1", "2", "3", "4", "5"]:
        tid = tokenizer.encode(s, add_special_tokens=False)
        if len(tid) != 1:
            # ìˆ«ìê°€ í•œ í† í°ì´ ì•„ë‹ ìˆ˜ë„ ìˆìŒ. ì´ ê²½ìš°ëŠ” ë³„ë„ ì²˜ë¦¬ í•„ìš”.
            raise ValueError(f"í† í°í™”ê°€ ì˜ˆìƒê³¼ ë‹¬ë¼ìš”: '{s}' -> {tid}. ìˆ«ìë§Œ 1í† í°ì´ ì•„ë‹ˆë©´ ë¡œì§ì„ ë°”ê¿”ì•¼ í•´ìš”.")
        choice_token_ids.append(tid[0])

    def preprocess_logits_for_metrics(logits, labels):
        """
        âœ… labelsì—ì„œ ì •ë‹µ í† í° ìœ„ì¹˜ë¥¼ ì°¾ì•„ì„œ ê·¸ ìœ„ì¹˜ì˜ logitsë§Œ ë½‘ì•„ì˜¨ë‹¤.
        ë°˜í™˜ shape: (batch, 5)  -> (1~5ì— ëŒ€í•œ ì ìˆ˜)
        """
        logits = logits if not isinstance(logits, tuple) else logits[0]  # (B, T, V)
        B, T, V = logits.shape

        # labels: (B, T)
        labels_t = labels

        # ì •ë‹µ ìœ„ì¹˜ idxë¥¼ ë°°ì¹˜ë§ˆë‹¤ êµ¬í•˜ê¸°: labels != -100 ì¸ ìœ„ì¹˜ ì¤‘ "ì²« ë²ˆì§¸"
        # (ì§€ê¸ˆì€ ë‹µ í† í° + <|im_end|> 2ê°œê°€ ë‚¨ì•„ìˆìœ¼ë‹ˆ ì²« ë²ˆì§¸ê°€ ë‹µ)
        ans_pos = []
        for i in range(B):
            positions = (labels_t[i] != -100).nonzero(as_tuple=False).squeeze(-1)
            if positions.numel() == 0:
                # í˜¹ì‹œ ì „ë¶€ -100ì´ë©´ ì•ˆì „í•˜ê²Œ ë§ˆì§€ë§‰ í† í°ìœ¼ë¡œ(ê·¼ë° ì´ëŸ° ìƒ˜í”Œì€ metricì—ì„œ ì‚¬ì‹¤ìƒ ë¬´ì˜ë¯¸)
                ans_pos.append(T - 1)
            else:
                ans_pos.append(int(positions[0].item()))
        ans_pos = torch.tensor(ans_pos, device=logits.device)  # (B,)

        # ë°°ì¹˜ ì¸ë±ì‹±ìœ¼ë¡œ ê° ìƒ˜í”Œì˜ ì •ë‹µ ìœ„ì¹˜ logitsì„ ë½‘ëŠ”ë‹¤: (B, V)
        batch_idx = torch.arange(B, device=logits.device)
        ans_logits = logits[batch_idx, ans_pos, :]  # (B, V)

        # ê·¸ ì¤‘ 1~5 í† í° idë§Œ ì¶”ì¶œ: (B, 5)
        ans_logits_5 = ans_logits[:, choice_token_ids]
        return ans_logits_5

    def compute_metrics(eval_pred):
        """
        preprocess_logits_for_metricsê°€ ì´ë¯¸ (B,5) logitsì„ ë„˜ê²¨ì¤Œ.
        labelsì—ì„œ ì •ë‹µ ìˆ«ìë„ ë˜‘ê°™ì´ labels ê¸°ë°˜ìœ¼ë¡œ ì¶”ì¶œí•´ì„œ ë¹„êµ.
        """
        logits_5, labels = eval_pred  # logits_5: (B,5)

        # labelsì—ì„œ ì •ë‹µ í† í° id ì¶”ì¶œ (labels != -100ì˜ ì²« ë²ˆì§¸ í† í°)
        B, T = labels.shape
        y_true = []
        for i in range(B):
            positions = np.where(labels[i] != -100)[0]
            if len(positions) == 0:
                y_true.append(0)
            else:
                tid = int(labels[i, positions[0]])
                # tid -> "1~5"ë¡œ ë§¤í•‘
                if tid in choice_token_ids:
                    y_true.append(choice_token_ids.index(tid))
                else:
                    # ì˜ˆìƒ ë°–ì´ë©´ 0 ì²˜ë¦¬
                    y_true.append(0)

        probs = torch.softmax(torch.tensor(logits_5), dim=-1)
        y_pred = torch.argmax(probs, dim=-1).cpu().numpy()

        return f1_metric.compute(predictions=y_pred, references=y_true, average="macro")

    return preprocess_logits_for_metrics, compute_metrics



# =============================================================================
# ë©”ì¸ í•™ìŠµ í•¨ìˆ˜
# =============================================================================

def train(config, resume_from_checkpoint: Optional[str] = None):
    """ëª¨ë¸ í•™ìŠµ ì‹¤í–‰"""

    print("=" * 60)
    print("ğŸš€ í•™ìŠµ ì‹œì‘")
    print("=" * 60)

    # 1. ì‹œë“œ ê³ ì •
    set_seed(config.training.seed)

    # 2. ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
    print("\nğŸ“‚ ë°ì´í„° ë¡œë“œ ì¤‘...")
    df = load_data(config.path.train_data)

    processed_dataset = process_dataset_for_training(df)


    print(f"  - ì´ ë°ì´í„° ìˆ˜: {len(df)}")

    #
    print(f"  - ì „ì²˜ë¦¬ ì™„ë£Œ: {len(processed_dataset)} samples")
    # print(f"ğŸ’½ Data Format{processed_dataset['messages'][0:4]}")

    # 3. ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
    print(f"\nğŸ¤– ëª¨ë¸ ë¡œë“œ ì¤‘: {config.model.model_name}")

    model, tokenizer = FastLanguageModel.from_pretrained(
        config.model.model_name,
        dtype=get_torch_dtype(config.model.torch_dtype),
        # trust_remote_code=config.model.trust_remote_code,
        load_in_4bit=True,
        max_seq_length=config.training.max_seq_length,   # âœ… ì¶”ê°€
    )

    tokenizer = setup_tokenizer(tokenizer)

    print("  - ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ")

    # 4. í† í°í™”
    print(f"\nğŸ“ í† í°í™” ì¤‘ (max_seq_length: {config.training.max_seq_length})...")
    tokenized_train = tokenize_dataset(
        processed_dataset,
        tokenizer,
        max_seq_length=config.training.max_seq_length
    )


    # í† í° í†µê³„
    # stats = get_token_statistics(tokenized_dataset, tokenizer)
    # print(f"  - Token ê¸¸ì´: min={stats['min']}, max={stats['max']}, mean={stats['mean']:.1f}")

    stats = get_token_statistics(tokenized_train, tokenizer)
    print(f"  - Token ê¸¸ì´(train): min={stats['min']}, max={stats['max']}, mean={stats['mean']:.1f}")

    # print(tokenized_dataset.column_names)
    # print(tokenized_dataset[0].keys())

    # 5. LoRA ì„¤ì •
    print(f"\nâš™ï¸ LoRA ì„¤ì •")
    model = FastLanguageModel.get_peft_model(
        model,
        r=config.lora.r,
        target_modules=config.lora.target_modules,
        lora_alpha=config.lora.lora_alpha,
        lora_dropout=config.lora.lora_dropout,
        bias=config.lora.bias,# âœ… ì—¬ê¸° 2ê°œ ì¶”ê°€
        use_gradient_checkpointing=config.lora.use_gradient_checkpointing,  # "unsloth"
        use_rslora=config.lora.use_rslora,

        random_state=config.training.seed,
        max_seq_length=config.training.max_seq_length,
        loftq_config=None
    )
    print(f"  - r: {config.lora.r}, alpha: {config.lora.lora_alpha}")
    print(f"  - target_modules: {config.lora.target_modules}")


    # 6. ë©”íŠ¸ë¦­ í•¨ìˆ˜ ìƒì„±
    preprocess_logits_for_metrics, compute_metrics = create_metric_functions(tokenizer)

    # 7. SFTConfig ì„¤ì •
    print(f"\nğŸ“‹ í•™ìŠµ ì„¤ì •")
    print(f"  - epochs: {config.training.num_train_epochs}")
    print(f"  - batch_size: {config.training.per_device_train_batch_size}")
    print(f"  - learning_rate: {config.training.learning_rate}")
    print(f"  - output_dir: {config.path.output_dir}")

    sft_config = UnslothTrainingArguments(
        # do_train=True,
        # do_eval=True,
        per_device_train_batch_size=config.training.per_device_train_batch_size,
        gradient_accumulation_steps=config.training.gradient_accumulation_steps,
        warmup_steps=config.training.warmup_steps,
        warmup_ratio=config.training.warmup_ratio,
        max_steps=config.training.max_steps,
        num_train_epochs=config.training.num_train_epochs,
        learning_rate=config.training.learning_rate,
        fp16=not is_bfloat16_supported(),
        bf16=is_bfloat16_supported(),
        logging_steps=config.training.logging_steps,
        optim=config.training.optim,
        weight_decay=config.training.weight_decay,
        lr_scheduler_type=config.training.lr_scheduler_type,
        seed = config.training.seed,
        output_dir=config.path.output_dir,

        per_device_eval_batch_size=config.training.per_device_eval_batch_size,

        # âœ… config ê°’ ê·¸ëŒ€ë¡œ ì‚¬ìš©
        save_strategy=config.training.save_strategy,
        eval_strategy=config.training.eval_strategy,
        save_total_limit=config.training.save_total_limit,

        # âœ… best ëª¨ë¸ ì €ì¥ (ë„¤ê°€ ì¶”ê°€í•œ 3ê°œê°€ ì—¬ê¸°ë¡œ ë“¤ì–´ì™€ì•¼ ì ìš©ë¨)
        load_best_model_at_end=config.training.load_best_model_at_end,
        metric_for_best_model=config.training.metric_for_best_model,
        greater_is_better=config.training.greater_is_better,
    )

    # 8. Trainer ìƒì„±
    trainer = UnslothTrainer(
        model=model,
        tokenizer = tokenizer,
        train_dataset=tokenized_train,
        args=sft_config,
    )

    resume_path = None
    if resume_from_checkpoint:
        if resume_from_checkpoint == "auto":
            resume_path = _get_latest_checkpoint(config.path.output_dir)
        else:
            resume_path = resume_from_checkpoint

        # ì‹¤ìˆ˜ ë°©ì§€: output_dir ìì²´ë¥¼ ë„£ì—ˆìœ¼ë©´ ìë™ìœ¼ë¡œ latestë¡œ ë³´ì •
        if resume_path and os.path.isdir(resume_path) and os.path.basename(resume_path) == os.path.basename(config.path.output_dir):
            resume_path = _get_latest_checkpoint(resume_path)

        if resume_path and (not os.path.isdir(resume_path)):
            raise FileNotFoundError(f"resume checkpoint not found: {resume_path}")

    # 9. í•™ìŠµ ì‹¤í–‰
    print("\n" + "=" * 60)
    print("ğŸƒ í•™ìŠµ ì‹¤í–‰ ì¤‘...")
    print("=" * 60)

    # train_result = trainer.train()

    if resume_path:
        print(f"\nğŸ” Resume from checkpoint: {resume_path}")
        train_result =trainer.train(resume_from_checkpoint=resume_path)
    else:
        train_result = trainer.train()

    print("best_ckpt:", trainer.state.best_model_checkpoint)
    print("best_metric:", trainer.state.best_metric)

    # trainer.save_model(config.path.output_dir)  # (ì„ íƒ) ìµœì¢… ëª¨ë¸ ì €ì¥


    # 10. ê²°ê³¼ ì¶œë ¥
    print("\n" + "=" * 60)
    print("âœ… í•™ìŠµ ì™„ë£Œ!")
    print("=" * 60)
    print(f"  - Total steps: {train_result.global_step}")
    print(f"  - Train loss: {train_result.training_loss:.4f}")
    print(f"  - ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ìœ„ì¹˜: {config.path.output_dir}")

    return trainer


# =============================================================================
# ë©”ì¸ ì‹¤í–‰
# =============================================================================

def main():
    parser = argparse.ArgumentParser(description="Train the model")
    parser.add_argument(
        "--exp",
        type=str,
        default=None,
        help="ì‹¤í—˜ ì´ë¦„ (large_context, more_lora, longer_training)"
    )
    parser.add_argument(
        "--output_dir",
        type=str,
        default=None,
        help="ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ê²½ë¡œ"
    )
    parser.add_argument(
        "--epochs",
        type=int,
        default=None,
        help="í•™ìŠµ ì—í­ ìˆ˜"
    )
    parser.add_argument(
        "--lr",
        type=float,
        default=None,
        help="í•™ìŠµë¥ "
    )
    parser.add_argument(
        "--max_seq_length",
        type=int,
        default=None,
        help="ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´"
    )

    # âœ… resume ì¸ì ì¶”ê°€
    parser.add_argument(
        "--resume_from_checkpoint",
        type=str,
        default=None,
        help="ì˜ˆ) ../results/.../checkpoint-256  ë˜ëŠ”  auto"
    )

    args = parser.parse_args()

    # ì„¤ì • ë¡œë“œ
    if args.exp:
        config = get_experiment_config(args.exp)
        print(f"ğŸ“Œ ì‹¤í—˜ ì„¤ì • ë¡œë“œ: {args.exp}")
    else:
        config = get_config()
        print("ğŸ“Œ ê¸°ë³¸ ì„¤ì • ì‚¬ìš©")

    # ëª…ë ¹ì¤„ ì¸ìë¡œ ì˜¤ë²„ë¼ì´ë“œ
    if args.output_dir:
        config.path.output_dir = args.output_dir
    if args.epochs:
        config.training.num_train_epochs = args.epochs
    if args.lr:
        config.training.learning_rate = args.lr
    if args.max_seq_length:
        config.training.max_seq_length = args.max_seq_length

    # í•™ìŠµ ì‹¤í–‰
    train(config, resume_from_checkpoint=args.resume_from_checkpoint)


if __name__ == "__main__":
    main()