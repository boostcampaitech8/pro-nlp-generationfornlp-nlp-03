"""
ì¶”ë¡  ìŠ¤í¬ë¦½íŠ¸
Usage:
    python inference.py --checkpoint outputs/checkpoint-4491
    python inference.py --checkpoint outputs/checkpoint-4491 --output my_output.csv
"""

import os
import argparse
import torch
import numpy as np
import pandas as pd
from tqdm import tqdm

from transformers import AutoTokenizer
from peft import AutoPeftModelForCausalLM

from config import get_config
from data_utils import load_data, process_dataset_for_inference, setup_tokenizer


# =============================================================================
# ì¶”ë¡  í•¨ìˆ˜
# =============================================================================

def inference(
    checkpoint_path: str,
    test_data_path: str,
    output_path: str,
    device: str = "cuda"
):
    """
    ëª¨ë¸ ì¶”ë¡  ì‹¤í–‰
    
    Args:
        checkpoint_path: í•™ìŠµëœ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ
        test_data_path: í…ŒìŠ¤íŠ¸ ë°ì´í„° ê²½ë¡œ
        output_path: ê²°ê³¼ ì €ì¥ ê²½ë¡œ
        device: ë””ë°”ì´ìŠ¤ ("cuda" or "cpu")
    """
    
    print("=" * 60)
    print("ğŸ”® ì¶”ë¡  ì‹œì‘")
    print("=" * 60)
    
    # 1. ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
    print(f"\nğŸ¤– ëª¨ë¸ ë¡œë“œ ì¤‘: {checkpoint_path}")
    
    model = AutoPeftModelForCausalLM.from_pretrained(
        checkpoint_path,
        trust_remote_code=True,
        device_map="auto",
    )
    
    tokenizer = AutoTokenizer.from_pretrained(
        checkpoint_path,
        trust_remote_code=True,
    )
    tokenizer = setup_tokenizer(tokenizer)
    print("  - ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ")
    
    # 2. í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ
    print(f"\nğŸ“‚ í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ ì¤‘: {test_data_path}")
    test_df = load_data(test_data_path)
    test_dataset = process_dataset_for_inference(test_df)
    print(f"  - í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ ìˆ˜: {len(test_dataset)}")
    
    # 3. ì¶”ë¡  ì‹¤í–‰
    print("\n" + "=" * 60)
    print("ğŸƒ ì¶”ë¡  ì‹¤í–‰ ì¤‘...")
    print("=" * 60)
    
    infer_results = []
    pred_choices_map = {0: "1", 1: "2", 2: "3", 3: "4", 4: "5"}
    
    model.eval()
    with torch.inference_mode():
        for data in tqdm(test_dataset, desc="Inference"):
            _id = data["id"]
            messages = data["messages"]
            len_choices = data["len_choices"]
            
            # í† í°í™”
            inputs = tokenizer.apply_chat_template(
                messages,
                tokenize=True,
                add_generation_prompt=True,
                return_tensors="pt",
            ).to(device)
            
            # ëª¨ë¸ ì¶”ë¡ 
            outputs = model(inputs)
            logits = outputs.logits[:, -1].flatten().cpu()
            
            # ì„ íƒì§€ í† í°ì˜ logit ì¶”ì¶œ
            target_logit_list = [
                logits[tokenizer.vocab[str(i + 1)]] 
                for i in range(len_choices)
            ]
            
            # Softmaxë¡œ í™•ë¥  ë³€í™˜
            probs = torch.nn.functional.softmax(
                torch.tensor(target_logit_list, dtype=torch.float32),
                dim=-1
            ).detach().cpu().numpy()
            
            # ìµœì¢… ì˜ˆì¸¡
            predict_value = pred_choices_map[np.argmax(probs)]
            infer_results.append({"id": _id, "answer": predict_value})
    
    # 4. ê²°ê³¼ ì €ì¥
    print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥ ì¤‘: {output_path}")
    result_df = pd.DataFrame(infer_results)
    result_df.to_csv(output_path, index=False)
    
    print("\n" + "=" * 60)
    print("âœ… ì¶”ë¡  ì™„ë£Œ!")
    print("=" * 60)
    print(f"  - ì´ ì˜ˆì¸¡ ìˆ˜: {len(infer_results)}")
    print(f"  - ì €ì¥ ìœ„ì¹˜: {output_path}")
    
    # ì˜ˆì¸¡ ë¶„í¬ ì¶œë ¥
    print("\nğŸ“Š ì˜ˆì¸¡ ë¶„í¬:")
    value_counts = result_df['answer'].value_counts().sort_index()
    for ans, count in value_counts.items():
        print(f"  - {ans}: {count} ({count/len(result_df)*100:.1f}%)")
    
    return result_df


def inference_with_generation(
    checkpoint_path: str,
    test_data_path: str,
    output_path: str,
    device: str = "cuda",
    max_new_tokens: int = 5
):
    """
    ìƒì„± ë°©ì‹ ì¶”ë¡  (generate í•¨ìˆ˜ ì‚¬ìš©)
    - logit ê¸°ë°˜ë³´ë‹¤ ëŠë¦¬ì§€ë§Œ, ë” ë³µì¡í•œ ì¶œë ¥ì— ì í•©
    """
    
    print("=" * 60)
    print("ğŸ”® ì¶”ë¡  ì‹œì‘ (Generation Mode)")
    print("=" * 60)
    
    # ëª¨ë¸ ë¡œë“œ
    print(f"\nğŸ¤– ëª¨ë¸ ë¡œë“œ ì¤‘: {checkpoint_path}")
    model = AutoPeftModelForCausalLM.from_pretrained(
        checkpoint_path,
        trust_remote_code=True,
        device_map="auto",
    )
    
    tokenizer = AutoTokenizer.from_pretrained(
        checkpoint_path,
        trust_remote_code=True,
    )
    tokenizer = setup_tokenizer(tokenizer)
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ
    print(f"\nğŸ“‚ í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ ì¤‘: {test_data_path}")
    test_df = load_data(test_data_path)
    test_dataset = process_dataset_for_inference(test_df)
    print(f"  - í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ ìˆ˜: {len(test_dataset)}")
    
    # ì¶”ë¡ 
    print("\nğŸƒ ì¶”ë¡  ì‹¤í–‰ ì¤‘...")
    infer_results = []
    
    model.eval()
    with torch.inference_mode():
        for data in tqdm(test_dataset, desc="Inference"):
            _id = data["id"]
            messages = data["messages"]
            
            inputs = tokenizer.apply_chat_template(
                messages,
                tokenize=True,
                add_generation_prompt=True,
                return_tensors="pt",
            ).to(device)
            
            # ìƒì„±
            outputs = model.generate(
                inputs,
                max_new_tokens=max_new_tokens,
                do_sample=False,  # Greedy decoding
                pad_token_id=tokenizer.pad_token_id,
            )
            
            # ë””ì½”ë”© (ìƒì„±ëœ ë¶€ë¶„ë§Œ)
            generated = outputs[0][inputs.shape[1]:]
            answer_text = tokenizer.decode(generated, skip_special_tokens=True).strip()
            
            # ì •ë‹µ ì¶”ì¶œ (ì²« ë²ˆì§¸ ìˆ«ì)
            answer = "1"  # ê¸°ë³¸ê°’
            for char in answer_text:
                if char in "12345":
                    answer = char
                    break
            
            infer_results.append({"id": _id, "answer": answer})
    
    # ê²°ê³¼ ì €ì¥
    result_df = pd.DataFrame(infer_results)
    result_df.to_csv(output_path, index=False)
    
    print(f"\nâœ… ì¶”ë¡  ì™„ë£Œ! ì €ì¥ ìœ„ì¹˜: {output_path}")
    
    return result_df


# =============================================================================
# ë©”ì¸ ì‹¤í–‰
# =============================================================================

def main():
    parser = argparse.ArgumentParser(description="Run inference")
    parser.add_argument(
        "--checkpoint", 
        type=str, 
        required=True,
        help="í•™ìŠµëœ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ"
    )
    parser.add_argument(
        "--test_data",
        type=str,
        default=None,
        help="í…ŒìŠ¤íŠ¸ ë°ì´í„° ê²½ë¡œ (ê¸°ë³¸: configì˜ test_data)"
    )
    parser.add_argument(
        "--output",
        type=str,
        default=None,
        help="ê²°ê³¼ ì €ì¥ ê²½ë¡œ (ê¸°ë³¸: configì˜ output_csv)"
    )
    parser.add_argument(
        "--mode",
        type=str,
        default="logit",
        choices=["logit", "generate"],
        help="ì¶”ë¡  ë°©ì‹ (logit: logit ê¸°ë°˜, generate: ìƒì„± ê¸°ë°˜)"
    )
    parser.add_argument(
        "--device",
        type=str,
        default="cuda",
        help="ë””ë°”ì´ìŠ¤ (cuda or cpu)"
    )
    
    args = parser.parse_args()
    
    # ì„¤ì • ë¡œë“œ
    config = get_config()
    
    test_data_path = args.test_data or config.path.test_data
    output_path = args.output or config.path.output_csv
    
    # ì¶”ë¡  ì‹¤í–‰
    if args.mode == "logit":
        inference(
            checkpoint_path=args.checkpoint,
            test_data_path=test_data_path,
            output_path=output_path,
            device=args.device,
        )
    else:
        inference_with_generation(
            checkpoint_path=args.checkpoint,
            test_data_path=test_data_path,
            output_path=output_path,
            device=args.device,
        )


if __name__ == "__main__":
    main()
