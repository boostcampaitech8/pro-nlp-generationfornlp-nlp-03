"""
ì¶”ë¡  ìŠ¤í¬ë¦½íŠ¸
Usage:
    python inference.py --checkpoint outputs/checkpoint-4491
    python inference.py --checkpoint outputs/checkpoint-4491 --output my_output.csv
"""

import unsloth
import os
import argparse
import torch
import numpy as np
import pandas as pd
from tqdm import tqdm

from sklearn.metrics import f1_score, accuracy_score, classification_report

from transformers import AutoTokenizer, TextStreamer
from unsloth import FastLanguageModel

from config.config import get_config
from utils.data_utils import load_data, process_dataset_for_inference, setup_tokenizer
from utils.analysis_utils import analyze_subject_accuracy
from utils.parser_utils import parse_answer


# =============================================================================
# ì¶”ë¡  í•¨ìˆ˜
# =============================================================================


def inference(
    config,
    checkpoint_path: str,
    test_data_path: str,
    output_path: str,
    device: str = "cuda",
):
    """
    ëª¨ë¸ ì¶”ë¡  ì‹¤í–‰

    Args:
        checkpoint_path: í•™ìŠµëœ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ
        test_data_path: í…ŒìŠ¤íŠ¸ ë°ì´í„° ê²½ë¡œ
        output_path: ê²°ê³¼ ì €ì¥ ê²½ë¡œ
        device: ë””ë°”ì´ìŠ¤ ("cuda" or "cpu")
    """

    print("=" * 60)
    print("ğŸ”® ì¶”ë¡  ì‹œì‘")
    print("=" * 60)

    # 1. ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
    print(f"\nğŸ¤– ëª¨ë¸ ë¡œë“œ ì¤‘: {checkpoint_path}")

    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name=config.model.model_name,
        max_seq_length=config.training.max_seq_length,
        dtype=torch.float16,
        load_in_4bit=True,
        adapter_name=checkpoint_path,
    )

    FastLanguageModel.for_inference(model)

    tokenizer = setup_tokenizer(tokenizer)  # Instruct ì‚¬ìš© ì‹œì—ëŠ” í•„ìš”ì—†ëŠ” ì½”ë“œ

    print("  - ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ")

    # 2. í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ
    print(f"\nğŸ“‚ í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ ì¤‘: {test_data_path}")
    test_df = load_data(test_data_path)
    test_dataset = process_dataset_for_inference(test_df)
    print(f"  - í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ ìˆ˜: {len(test_dataset)}")

    # 3. ì¶”ë¡  ì‹¤í–‰
    print("\n" + "=" * 60)
    print("ğŸƒ ì¶”ë¡  ì‹¤í–‰ ì¤‘...")
    print("=" * 60)

    infer_results = []
    pred_choices_map = {0: "1", 1: "2", 2: "3", 3: "4", 4: "5"}

    MAX_CTX = model.max_seq_length
    print(f"\nğŸ“MAX Seqence Lenght: {MAX_CTX}")

    # validation ë°ì´í„° ì¸ ê²½ìš°ì— ì‚¬ìš© - ìƒˆë¡œ ì¶”ê°€
    topics = []
    labels = []
    types = []
    types_topics = []

    model.eval()
    with torch.inference_mode():
        for data in tqdm(test_dataset, desc="Inference"):
            _id = data["id"]
            messages = data["messages"]
            len_choices = data["len_choices"]

            # validation ë°ì´í„° ì¸ ê²½ìš°ì— ì‚¬ìš© - ìƒˆë¡œ ì¶”ê°€
            if data["topic"] is not None and data["label"] is not None:
                topics.append(data["topic"])
                labels.append(str(data["label"]))
                types.append(data["type"])
                types_topics.append(data["stratify_key"])

            # í† í°í™”
            inputs = tokenizer.apply_chat_template(
                messages,
                tokenize=True,
                add_generation_prompt=True,
                return_tensors="pt",
                MAX_CTX=model.max_seq_length,
            ).to(device)

            # ëª¨ë¸ ì¶”ë¡ 
            outputs = model(inputs)
            logits = outputs.logits[:, -1].flatten().cpu()

            # ì„ íƒì§€ í† í°ì˜ logit ì¶”ì¶œ
            target_logit_list = [
                logits[tokenizer.vocab[str(i + 1)]] for i in range(len_choices)
            ]

            # Softmaxë¡œ í™•ë¥  ë³€í™˜
            probs = (
                torch.nn.functional.softmax(
                    torch.tensor(target_logit_list, dtype=torch.float32), dim=-1
                )
                .detach()
                .cpu()
                .numpy()
            )

            # ìµœì¢… ì˜ˆì¸¡
            if probs.size == 0:
                predict_value = "1"
                print("\nğŸš« Null Probaility")
            else:
                predict_value = pred_choices_map[np.argmax(probs)]
                logit_confidence = float(np.max(probs))

                # 1ë“± í™•ë¥ ê³¼ 2ë“± í™•ë¥  ê°„ì˜ ì°¨ì´ ê³„ì‚°
                sorted_probs = np.sort(probs)[::-1]
                logit_margin = float(sorted_probs[0] - sorted_probs[1])

            infer_results.append(
                {
                    "id": _id,
                    "answer": predict_value,
                    "logit_confidence": logit_confidence,
                    "logit_margin": logit_margin,
                }
            )

    # 4. ê²°ê³¼ ì €ì¥
    print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥ ì¤‘: {output_path}")
    result_df = pd.DataFrame(infer_results)
    result_df.to_csv(output_path, index=False)

    print("\n" + "=" * 60)
    print("âœ… ì¶”ë¡  ì™„ë£Œ!")
    print("=" * 60)
    print(f"  - ì´ ì˜ˆì¸¡ ìˆ˜: {len(infer_results)}")
    print(f"  - ì €ì¥ ìœ„ì¹˜: {output_path}")

    # ì˜ˆì¸¡ ë¶„í¬ ì¶œë ¥
    print("\nğŸ“Š ì˜ˆì¸¡ ë¶„í¬:")
    value_counts = result_df["answer"].value_counts().sort_index()
    for ans, count in value_counts.items():
        print(f"  - {ans}: {count} ({count/len(result_df)*100:.1f}%)")

    # validation ë°ì´í„° ì¸ ê²½ìš°ì— ì‚¬ìš© - ìƒˆë¡œ ì¶”ê°€
    if len(topics) != 0:

        df = result_df.copy()

        df["label"] = labels
        df["topic"] = topics
        df["type"] = types
        df["stratify_key"] = types_topics

        f1_macro = f1_score(df["label"], df["answer"], average="macro")
        f1_weighted = f1_score(df["label"], df["answer"], average="weighted")
        acc = accuracy_score(df["label"], df["answer"])
        print("\nğŸ“‘ Score Report:")
        print(f"Accuracy     : {acc:.4f}")
        print(f"F1-macro     : {f1_macro:.4f}")
        print(f"F1-weighted  : {f1_weighted:.4f}")

        visual_path = f"{config.path.visualize_dir}/{config.model.model_name}"
        result_topic = analyze_subject_accuracy(
            df,
            true_col="label",
            pred_col="answer",
            topic_col="topic",
            save_dir=visual_path,
        )

        result_type = analyze_subject_accuracy(
            df,
            true_col="label",
            pred_col="answer",
            topic_col="type",
            save_dir=visual_path,
        )

        result_type_topic = analyze_subject_accuracy(
            df,
            true_col="label",
            pred_col="answer",
            topic_col="stratify_key",
            save_dir=visual_path,
        )

    return result_df


def inference_with_generation(
    config,
    checkpoint_path: str,
    test_data_path: str,
    output_path: str,
    device: str = "cuda",
    max_new_tokens: int = 768,
):
    """
    ìƒì„± ë°©ì‹ ì¶”ë¡  (generate í•¨ìˆ˜ ì‚¬ìš©)
    - logit ê¸°ë°˜ë³´ë‹¤ ëŠë¦¬ì§€ë§Œ, ë” ë³µì¡í•œ ì¶œë ¥ì— ì í•©
    """

    print("=" * 60)
    print("ğŸ”® ì¶”ë¡  ì‹œì‘ (Generation Mode)")
    print("=" * 60)

    # ëª¨ë¸ ë¡œë“œ
    print(f"\nğŸ¤– ëª¨ë¸ ë¡œë“œ ì¤‘: {checkpoint_path}")

    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name=config.model.model_name,
        max_seq_length=config.training.max_seq_length,
        dtype=torch.float16,
        load_in_4bit=True,
        adapter_name=checkpoint_path,
    )

    FastLanguageModel.for_inference(model)
    text_streamer = TextStreamer(tokenizer)

    tokenizer = setup_tokenizer(tokenizer)

    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ
    print(f"\nğŸ“‚ í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ ì¤‘: {test_data_path}")
    test_df = load_data(test_data_path)
    test_dataset = process_dataset_for_inference(test_df)
    print(f"  - í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ ìˆ˜: {len(test_dataset)}")

    # ì¶”ë¡ 
    print("\nğŸƒ ì¶”ë¡  ì‹¤í–‰ ì¤‘...")
    infer_results = []

    MAX_CTX = model.max_seq_length
    print(f"\nğŸ“MAX Seqence Lenght: {MAX_CTX}")

    # validation ë°ì´í„° ì¸ ê²½ìš°ì— ì‚¬ìš© - ìƒˆë¡œ ì¶”ê°€
    topics = []
    labels = []
    types = []
    types_topics = []

    model.eval()
    with torch.inference_mode():
        for data in tqdm(test_dataset, desc="Inference"):
            _id = data["id"]
            messages = data["messages"]

            # validation ë°ì´í„° ì¸ ê²½ìš°ì— ì‚¬ìš© - ìƒˆë¡œ ì¶”ê°€
            if data["topic"] is not None and data["label"] is not None:
                topics.append(data["topic"])
                labels.append(str(data["label"]))
                types.append(data["type"])
                types_topics.append(data["stratify_key"])

            # Claudeë¡œ ì¶”ê°€(ê²½ê³  ë– ì„œ ì¶”ê°€í•œ ì½”ë“œ)
            inputs = tokenizer.apply_chat_template(
                messages,
                tokenize=True,
                add_generation_prompt=True,
                return_tensors="pt",
            ).to(device)

            # attention_mask ìƒì„±
            attention_mask = torch.ones_like(inputs).to(device)

            outputs = model.generate(
                input_ids=inputs,
                attention_mask=attention_mask,
                # streamer=text_streamer, # ì½˜ì†” ì¶œë ¥ìš©
                max_new_tokens=max_new_tokens,
                do_sample=False,
                pad_token_id=tokenizer.pad_token_id,
            )

            # ë””ì½”ë”© (ìƒì„±ëœ ë¶€ë¶„ë§Œ)
            generated = outputs[0][inputs.shape[1] :]
            answer_text = tokenizer.decode(generated, skip_special_tokens=True).strip()

            answer = parse_answer(answer_text)
            if answer is None:
                answer = "-1"
            # print("\nğŸ””Parser Answer:", answer)

            infer_results.append(
                {"id": _id, "answer": answer, "raw_output": answer_text}
            )

    # ê²°ê³¼ ì €ì¥
    result_df = pd.DataFrame(infer_results)
    result_df.to_csv(output_path, index=False)

    print(f"\nâœ… ì¶”ë¡  ì™„ë£Œ! ì €ì¥ ìœ„ì¹˜: {output_path}")

    # ì˜ˆì¸¡ ë¶„í¬ ì¶œë ¥
    print("\nğŸ“Š ì˜ˆì¸¡ ë¶„í¬:")
    value_counts = result_df["answer"].value_counts().sort_index()
    for ans, count in value_counts.items():
        print(f"  - {ans}: {count} ({count/len(result_df)*100:.1f}%)")

    # validation ë°ì´í„° ì¸ ê²½ìš°ì— ì‚¬ìš© - ìƒˆë¡œ ì¶”ê°€
    if len(topics) != 0:

        df = result_df.copy()

        df["label"] = labels
        df["topic"] = topics
        df["type"] = types
        df["stratify_key"] = types_topics

        f1_macro = f1_score(df["label"], df["answer"], average="macro")
        f1_weighted = f1_score(df["label"], df["answer"], average="weighted")
        acc = accuracy_score(df["label"], df["answer"])
        print("\nğŸ“‘ Score Report:")
        print(f"Accuracy     : {acc:.4f}")
        print(f"F1-macro     : {f1_macro:.4f}")
        print(f"F1-weighted  : {f1_weighted:.4f}")

        visual_path = f"{config.path.visualize_dir}/{config.model.model_name}"
        result_topic = analyze_subject_accuracy(
            df,
            true_col="label",
            pred_col="answer",
            topic_col="topic",
            save_dir=visual_path,
        )

        result_type = analyze_subject_accuracy(
            df,
            true_col="label",
            pred_col="answer",
            topic_col="type",
            save_dir=visual_path,
        )

        result_type_topic = analyze_subject_accuracy(
            df,
            true_col="label",
            pred_col="answer",
            topic_col="stratify_key",
            save_dir=visual_path,
        )

    return result_df


# =============================================================================
# ë©”ì¸ ì‹¤í–‰
# =============================================================================


def main():
    # ì„¤ì • ë¡œë“œ
    config = get_config()

    parser = argparse.ArgumentParser(description="Run inference")
    parser.add_argument(
        "--checkpoint",
        type=str,
        default=config.model.model_name,
        # required=True,
        help="í•™ìŠµëœ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ",
    )
    parser.add_argument(
        "--test_data",
        type=str,
        default=None,
        help="í…ŒìŠ¤íŠ¸ ë°ì´í„° ê²½ë¡œ (ê¸°ë³¸: configì˜ test_data)",
    )
    parser.add_argument(
        "--output",
        type=str,
        default=None,
        help="ê²°ê³¼ ì €ì¥ ê²½ë¡œ (ê¸°ë³¸: configì˜ output_csv)",
    )
    parser.add_argument(
        "--mode",
        type=str,
        default="logit",
        choices=["logit", "generate"],
        help="ì¶”ë¡  ë°©ì‹ (logit: logit ê¸°ë°˜, generate: ìƒì„± ê¸°ë°˜)",
    )
    parser.add_argument(
        "--device", type=str, default="cuda", help="ë””ë°”ì´ìŠ¤ (cuda or cpu)"
    )

    args = parser.parse_args()

    test_data_path = args.test_data or config.path.test_data
    output_path = args.output or config.path.output_csv

    # ì¶”ë¡  ì‹¤í–‰
    if args.mode == "logit":
        inference(
            config,
            checkpoint_path=args.checkpoint,
            test_data_path=test_data_path,
            output_path=output_path,
            device=args.device,
        )
    else:
        inference_with_generation(
            config,
            checkpoint_path=args.checkpoint,
            test_data_path=test_data_path,
            output_path=output_path,
            device=args.device,
        )


if __name__ == "__main__":
    main()
