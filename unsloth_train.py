"""
í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
Usage:
    python train.py
    python train.py --exp large_context
"""
import unsloth
import os
import argparse
import torch
import numpy as np
import random
import evaluate

from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments
from trl import SFTTrainer, SFTConfig
from peft import LoraConfig
from unsloth import FastLanguageModel, is_bfloat16_supported

from config import get_config, get_experiment_config
from data_utils import (
    load_data, 
    process_dataset_for_training, 
    setup_tokenizer, 
    tokenize_dataset,
    get_token_statistics
)


# =============================================================================
# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# =============================================================================

def set_seed(seed: int):
    """ì¬í˜„ì„±ì„ ìœ„í•œ ì‹œë“œ ê³ ì •"""
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    np.random.seed(seed)
    random.seed(seed)
    print(f"âœ“ Seed ê³ ì •: {seed}")


def get_torch_dtype(dtype_str: str):
    """ë¬¸ìì—´ì„ torch dtypeìœ¼ë¡œ ë³€í™˜"""
    dtype_map = {
        "float16": torch.float16,
        "bfloat16": torch.bfloat16,
        "float32": torch.float32,
    }
    return dtype_map.get(dtype_str, torch.float16)


# =============================================================================
# ë©”íŠ¸ë¦­ í•¨ìˆ˜
# =============================================================================

def create_metric_functions(tokenizer):
    """ë©”íŠ¸ë¦­ ê³„ì‚° í•¨ìˆ˜ë“¤ ìƒì„±"""
    
    f1_metric = evaluate.load("f1")
    int_output_map = {"1": 0, "2": 1, "3": 2, "4": 3, "5": 4}
    
    def preprocess_logits_for_metrics(logits, labels):
        """ì •ë‹µ í† í° ìœ„ì¹˜ì˜ logitsë§Œ ì¶”ì¶œ"""
        logits = logits if not isinstance(logits, tuple) else logits[0]
        logit_idx = [
            tokenizer.vocab["1"], 
            tokenizer.vocab["2"], 
            tokenizer.vocab["3"], 
            tokenizer.vocab["4"], 
            tokenizer.vocab["5"]
        ]
        logits = logits[:, -2, logit_idx]  # -2: answer token, -1: eos token
        return logits

    def compute_metrics(evaluation_result):
        """ì •í™•ë„ ê³„ì‚°"""
        logits, labels = evaluation_result

        # í† í°í™”ëœ ë ˆì´ë¸” ë””ì½”ë”©
        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
        labels = tokenizer.batch_decode(labels, skip_special_tokens=True)
        labels = list(map(lambda x: x.split("<end_of_turn>")[0].strip(), labels))
        labels = list(map(lambda x: int_output_map.get(x, 0), labels))

        # Softmaxë¡œ í™•ë¥  ë³€í™˜
        probs = torch.nn.functional.softmax(torch.tensor(logits), dim=-1)
        predictions = np.argmax(probs, axis=-1)

        # ì •í™•ë„ ê³„ì‚°
        macro_f1 = f1_metric.compute(predictions=predictions, references=labels, average="macro")
        return macro_f1
    
    return preprocess_logits_for_metrics, compute_metrics


# =============================================================================
# ë©”ì¸ í•™ìŠµ í•¨ìˆ˜
# =============================================================================

def train(config):
    """ëª¨ë¸ í•™ìŠµ ì‹¤í–‰"""
    
    print("=" * 60)
    print("ğŸš€ í•™ìŠµ ì‹œì‘")
    print("=" * 60)
    
    # 1. ì‹œë“œ ê³ ì •
    set_seed(config.training.seed)
    
    # 2. ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
    print("\nğŸ“‚ ë°ì´í„° ë¡œë“œ ì¤‘...")
    df = load_data(config.path.train_data)
    print(f"  - ì´ ë°ì´í„° ìˆ˜: {len(df)}")
    
    processed_dataset = process_dataset_for_training(df)
    print(f"  - ì „ì²˜ë¦¬ ì™„ë£Œ: {len(processed_dataset)} samples")
    
    # 3. ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
    print(f"\nğŸ¤– ëª¨ë¸ ë¡œë“œ ì¤‘: {config.model.model_name}")
    
    model, tokenizer = FastLanguageModel.from_pretrained(
        config.model.model_name,
        dtype=get_torch_dtype(config.model.torch_dtype),
        # trust_remote_code=config.model.trust_remote_code,
        load_in_4bit=True
    )
    
    tokenizer = setup_tokenizer(tokenizer)  # Instruct ì‚¬ìš© ì‹œì—ëŠ” í•„ìš”ì—†ëŠ” ì½”ë“œ
    
    
    print("  - ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ")
    
    # 4. í† í°í™”
    print(f"\nğŸ“ í† í°í™” ì¤‘ (max_seq_length: {config.training.max_seq_length})...")
    tokenized_dataset = tokenize_dataset(
        processed_dataset, 
        tokenizer, 
        max_seq_length=config.training.max_seq_length
    )
    
    # Train/Eval ë¶„í• 
    split_dataset = tokenized_dataset.train_test_split(
        test_size=config.training.test_size, 
        seed=config.training.seed
    )
    train_dataset = split_dataset['train']
    eval_dataset = split_dataset['test']
    
    print(f"  - Train: {len(train_dataset)} samples")
    print(f"  - Eval: {len(eval_dataset)} samples")
    
    # í† í° í†µê³„
    stats = get_token_statistics(train_dataset, tokenizer)
    print(f"  - Token ê¸¸ì´: min={stats['min']}, max={stats['max']}, mean={stats['mean']:.1f}")
    
    # 5. LoRA ì„¤ì •
    print(f"\nâš™ï¸ LoRA ì„¤ì •")
    model = FastLanguageModel.get_peft_model(
        model,
        r=config.lora.r,
        target_modules=config.lora.target_modules,
        lora_alpha=config.lora.lora_alpha,
        lora_dropout=config.lora.lora_dropout,
        bias=config.lora.bias,
        use_gradient_checkpointing = "unsloth",
        random_state=config.training.seed,
        max_seq_length=config.training.max_seq_length,
        use_rslora=False,
        loftq_config=None
    )
    print(f"  - r: {config.lora.r}, alpha: {config.lora.lora_alpha}")
    print(f"  - target_modules: {config.lora.target_modules}")
    
    # 6. Data Collator ì„¤ì •
    
    response_template = "<start_of_turn>model"
    
    """
    data_collator = DataCollatorForCompletionOnlyLM(
        response_template=response_template,
        tokenizer=tokenizer,
    )
    """
    
    # 7. ë©”íŠ¸ë¦­ í•¨ìˆ˜ ìƒì„±
    preprocess_logits_for_metrics, compute_metrics = create_metric_functions(tokenizer)
    
    # 8. SFTConfig ì„¤ì •
    print(f"\nğŸ“‹ í•™ìŠµ ì„¤ì •")
    print(f"  - epochs: {config.training.num_train_epochs}")
    print(f"  - batch_size: {config.training.per_device_train_batch_size}")
    print(f"  - learning_rate: {config.training.learning_rate}")
    print(f"  - output_dir: {config.path.output_dir}")
    
    sft_config = SFTConfig(
        # do_train=True,
        # do_eval=True,
        per_device_train_batch_size=config.training.per_device_train_batch_size,
        gradient_accumulation_steps=config.training.gradient_accumulation_steps,
        warmup_steps=config.training.warmup_steps,
        # max_steps=config.training.max_steps,
        num_train_epochs=config.training.num_train_epochs,
        learning_rate=config.training.learning_rate,
        fp16=not is_bfloat16_supported(),
        bf16=is_bfloat16_supported(),
        logging_steps=config.training.logging_steps,
        optim=config.training.optim,
        weight_decay=config.training.weight_decay,
        lr_scheduler_type=config.training.lr_scheduler_type,
        seed = config.training.seed,
        output_dir=config.path.output_dir,
    
        per_device_eval_batch_size=config.training.per_device_eval_batch_size,
    )
    
    # 9. Trainer ìƒì„±
    trainer = SFTTrainer(
        model=model,
        tokenizer = tokenizer,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        # data_collator=data_collator,
        compute_metrics=compute_metrics,
        preprocess_logits_for_metrics=preprocess_logits_for_metrics,
        args=sft_config,
    )
    
    # 10. í•™ìŠµ ì‹¤í–‰
    print("\n" + "=" * 60)
    print("ğŸƒ í•™ìŠµ ì‹¤í–‰ ì¤‘...")
    print("=" * 60)
    
    train_result = trainer.train()
    
    # 11. ê²°ê³¼ ì¶œë ¥
    print("\n" + "=" * 60)
    print("âœ… í•™ìŠµ ì™„ë£Œ!")
    print("=" * 60)
    print(f"  - Total steps: {train_result.global_step}")
    print(f"  - Train loss: {train_result.training_loss:.4f}")
    print(f"  - ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ìœ„ì¹˜: {config.path.output_dir}")
    
    return trainer


# =============================================================================
# ë©”ì¸ ì‹¤í–‰
# =============================================================================

def main():
    parser = argparse.ArgumentParser(description="Train the model")
    parser.add_argument(
        "--exp", 
        type=str, 
        default=None,
        help="ì‹¤í—˜ ì´ë¦„ (large_context, more_lora, longer_training)"
    )
    parser.add_argument(
        "--output_dir",
        type=str,
        default=None,
        help="ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ê²½ë¡œ"
    )
    parser.add_argument(
        "--epochs",
        type=int,
        default=None,
        help="í•™ìŠµ ì—í­ ìˆ˜"
    )
    parser.add_argument(
        "--lr",
        type=float,
        default=None,
        help="í•™ìŠµë¥ "
    )
    parser.add_argument(
        "--max_seq_length",
        type=int,
        default=None,
        help="ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´"
    )
    
    args = parser.parse_args()
    
    # ì„¤ì • ë¡œë“œ
    if args.exp:
        config = get_experiment_config(args.exp)
        print(f"ğŸ“Œ ì‹¤í—˜ ì„¤ì • ë¡œë“œ: {args.exp}")
    else:
        config = get_config()
        print("ğŸ“Œ ê¸°ë³¸ ì„¤ì • ì‚¬ìš©")
    
    # ëª…ë ¹ì¤„ ì¸ìë¡œ ì˜¤ë²„ë¼ì´ë“œ
    if args.output_dir:
        config.path.output_dir = args.output_dir
    if args.epochs:
        config.training.num_train_epochs = args.epochs
    if args.lr:
        config.training.learning_rate = args.lr
    if args.max_seq_length:
        config.training.max_seq_length = args.max_seq_length
    
    # í•™ìŠµ ì‹¤í–‰
    train(config)


if __name__ == "__main__":
    main()